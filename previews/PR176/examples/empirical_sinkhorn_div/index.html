<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sinkhorn divergences · OptimalTransport.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://juliaoptimaltransport.github.io/OptimalTransport.jl/examples/empirical_sinkhorn_div/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><script src="../../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="OptimalTransport.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">OptimalTransport.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../OneDimension/">One-Dimensional Cases</a></li><li><a class="tocitem" href="../basic/">Basics</a></li><li class="is-active"><a class="tocitem" href>Sinkhorn divergences</a></li><li><a class="tocitem" href="../nmf/">Wasserstein non-negative matrix factorisation</a></li><li><a class="tocitem" href="../variational/">Variational problems</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Sinkhorn divergences</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sinkhorn divergences</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaOptimalTransport/OptimalTransport.jl/blob/master/examples/empirical_sinkhorn_div/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Sinkhorn-divergences"><a class="docs-heading-anchor" href="#Sinkhorn-divergences">Sinkhorn divergences</a><a id="Sinkhorn-divergences-1"></a><a class="docs-heading-anchor-permalink" href="#Sinkhorn-divergences" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/JuliaOptimalTransport/OptimalTransport.jl/blob/gh-pages/previews/PR176/examples/empirical_sinkhorn_div.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p><em>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/JuliaOptimalTransport/OptimalTransport.jl/blob/master/examples/empirical_sinkhorn_div/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/JuliaOptimalTransport/OptimalTransport.jl/blob/gh-pages/previews/PR176/examples/empirical_sinkhorn_div.ipynb">nbviewer</a>.</em></p><p>In this tutorial we provide a minimal example for using the Sinkhorn divergence as a loss function [FSV+19] on empirical distributions. [FSV+19]: Feydy, Jean, et al. &quot;Interpolating between optimal transport and MMD using Sinkhorn divergences.&quot; The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.</p><p>While entropy-regularised optimal transport <span>$\operatorname{OT}_{\varepsilon}(\cdot, \cdot)$</span> is commonly used as a loss function, it suffers from a problem of <em>bias</em>: namely that <span>$\nu \mapsto \operatorname{OT}_{\varepsilon}(\mu, \nu)$</span> is <em>not</em> minimised at <span>$\nu = \mu$</span>.</p><p>A fix to this problem is proposed by Genevay et al [GPC18] and subsequently Feydy et al. [FSV+19], which introduce the <em>Sinkhorn divergence</em> between two measures <span>$\mu$</span> and <span>$\nu$</span>, defined as</p><p class="math-container">\[\operatorname{S}_{\varepsilon}(\mu, \nu) = \operatorname{OT}_{\varepsilon}(\mu, \nu) - \frac{1}{2} \operatorname{OT}_{\varepsilon}(\mu, \mu) - \frac{1}{2} \operatorname{OT}_{\varepsilon}(\nu, \nu).\]</p><p>In the above, we have followed the convention taken by Feydy et al. and included the entropic regularisation in the definition of <span>$\operatorname{OT}_\varepsilon$</span>. [GPC18]: Aude Genevay, Gabriel Peyré, Marco Cuturi, Learning Generative Models with Sinkhorn Divergences, Proceedings of the Twenty-First International Conference on Artficial Intelligence and Statistics, (AISTATS) 21, 2018 [FSV+19]: Feydy, Jean, et al. &quot;Interpolating between optimal transport and MMD using Sinkhorn divergences.&quot; The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.</p><p>Like the Sinkhorn loss, the Sinkhorn divergence is smooth and convex in both of its arguments. However, the Sinkhorn divergence is unbiased – i.e. <span>$S_{\varepsilon}(\mu, \nu) = 0$</span> iff <span>$\mu = \nu$</span>.</p><p>Unlike previous examples, here we demonstrate a learning problem similar to Figure 1 of Feydy et al. over <em>empirical measures</em>, i.e. measures that have the form <span>$\mu = \frac{1}{N} \sum_{i = 1}^{N} \delta_{x_i}$</span> where <span>$\delta_x$</span> is the Dirac delta function at <span>$x$</span>.</p><p>We first load packages.</p><pre><code class="language-julia hljs">using OptimalTransport
using ReverseDiff
using Distributions
using LinearAlgebra
using Distances
using Plots
using Logging
using Optim</code></pre><p>As a ground truth distribution, we set <span>$\rho$</span> to be a Gaussian mixture model with <code>k = 3</code> components, equally spaced around a circle, and sample an empirical distribution of size <span>$N$</span>, <span>$\mu \sim \rho$</span>.</p><pre><code class="language-julia hljs">k = 3
d = 2
θ = π * range(0, 2(1 - 1 / k); length=k)
μ = 2 * hcat(sin.(θ), cos.(θ))
ρ = MixtureModel(MvNormal[MvNormal(x, 0.25 * I) for x in eachrow(μ)])
N = 100
μ_spt = rand(ρ, N)&#39;
scatter(μ_spt[:, 1], μ_spt[:, 2]; markeralpha=0.25, title=raw&quot;$\mu$&quot;)</code></pre><p><img src="../2406911020.svg" alt/></p><p>Now, suppose we want to approximate <span>$\mu$</span> with another empirical distribution <span>$\nu$</span>, i.e. we want to minimise <span>$\nu \mapsto \operatorname{S}_{\varepsilon}(\mu, \nu)$</span> over possible empirical distributions <span>$\nu$</span>. In this case we have <span>$M$</span> particles in <span>$\nu$</span>, which we initialise following a Gaussian distribution.</p><pre><code class="language-julia hljs">M = 100
ν_spt = rand(M, d);</code></pre><p>Assign uniform weights to the Diracs in each empirical distribution.</p><pre><code class="language-julia hljs">μ = fill(1 / N, N)
ν = fill(1 / M, M);</code></pre><p>Since <span>$\mu$</span> is fixed, we pre-compute the cost matrix <span>$C_{\mu}$</span>.</p><pre><code class="language-julia hljs">C_μ = pairwise(SqEuclidean(), μ_spt&#39;);</code></pre><p>Define the loss function to minimise, where <code>x</code> specifies the locations of the Diracs in <span>$\nu$</span>.</p><p>We will be using <code>ReverseDiff</code> with a precompiled tape. For this reason, we need the Sinkhorn algorithm to perform a fixed number of (e.g. 50) iterations. Currently, this can be achieved by setting <code>maxiter = 50</code> and <code>atol = rtol = 0</code> in calls to <code>sinkhorn</code> and <code>sinkhorn_divergence</code>.</p><pre><code class="language-julia hljs">function loss(x, ε)
    C_μν = pairwise(SqEuclidean(), μ_spt&#39;, x&#39;)
    C_ν = pairwise(SqEuclidean(), x&#39;)
    return sinkhorn_divergence(
        μ, ν, C_μν, C_μ, C_ν, ε; maxiter=50, atol=rtol = 0, regularization=true
    )
end</code></pre><pre><code class="nohighlight hljs">loss (generic function with 1 method)</code></pre><p>Set entropy regularisation parameter</p><pre><code class="language-julia hljs">ε = 1.0;</code></pre><p>Use ReverseDiff with a precompiled tape and Optim.jl to minimise <span>$\nu \mapsto \operatorname{S}_{\varepsilon}(\mu, \nu)$</span>. Note that this is problem is <em>not</em> convex, so we find a local minimium.</p><pre><code class="language-julia hljs">const loss_tape = ReverseDiff.GradientTape(x -&gt; loss(x, ε), ν_spt)
const compiled_loss_tape = ReverseDiff.compile(loss_tape)
opt = with_logger(SimpleLogger(stderr, Logging.Error)) do
    optimize(
        x -&gt; loss(x, ε),
        (∇, x) -&gt; ReverseDiff.gradient!(∇, compiled_loss_tape, x),
        ν_spt,
        GradientDescent(),
        Optim.Options(; iterations=10, g_tol=1e-6, show_trace=true),
    )
end
ν_opt = Optim.minimizer(opt)
plt1 = scatter(μ_spt[:, 1], μ_spt[:, 2]; markeralpha=0.25, title=&quot;Sinkhorn divergence&quot;)
scatter!(plt1, ν_opt[:, 1], ν_opt[:, 2]);</code></pre><pre><code class="nohighlight hljs">Iter     Function value   Gradient norm 
     0     3.499954e+00     4.261129e-02
 * time: 0.1020200252532959
     1     2.499118e-01     1.301351e-02
 * time: 2.9260292053222656
     2     1.966291e-02     5.290499e-03
 * time: 3.4586989879608154
     3     8.248033e-03     2.474341e-03
 * time: 4.265523195266724
     4     4.021280e-03     2.372214e-03
 * time: 4.835966110229492
     5     1.861874e-03     9.467070e-04
 * time: 5.763408184051514
     6     1.094097e-03     8.251763e-04
 * time: 6.350448131561279
     7     7.623626e-04     4.388443e-04
 * time: 7.14060115814209
     8     6.099758e-04     3.098383e-04
 * time: 7.755726099014282
     9     5.117904e-04     2.947444e-04
 * time: 8.617134094238281
    10     4.437907e-04     2.593877e-04
 * time: 9.2673180103302
</code></pre><p>For comparison, let us do the same computation again, but this time we want to minimise <span>$\nu \mapsto \operatorname{OT}_{\varepsilon}(\mu, \nu)$</span>.</p><pre><code class="language-julia hljs">function loss_biased(x, ε)
    C_μν = pairwise(SqEuclidean(), μ_spt&#39;, x&#39;)
    return sinkhorn2(μ, ν, C_μν, ε; maxiter=50, atol=rtol = 0, regularization=true)
end
const loss_biased_tape = ReverseDiff.GradientTape(x -&gt; loss_biased(x, ε), ν_spt)
const compiled_loss_biased_tape = ReverseDiff.compile(loss_biased_tape)
opt_biased = with_logger(SimpleLogger(stderr, Logging.Error)) do
    optimize(
        x -&gt; loss_biased(x, ε),
        (∇, x) -&gt; ReverseDiff.gradient!(∇, compiled_loss_biased_tape, x),
        ν_spt,
        GradientDescent(),
        Optim.Options(; iterations=10, g_tol=1e-6, show_trace=true),
    )
end
ν_opt_biased = Optim.minimizer(opt_biased)
plt2 = scatter(μ_spt[:, 1], μ_spt[:, 2]; markeralpha=0.25, title=&quot;Sinkhorn loss&quot;)
scatter!(plt2, ν_opt_biased[:, 1], ν_opt_biased[:, 2]);</code></pre><pre><code class="nohighlight hljs">Iter     Function value   Gradient norm 
     0    -4.618377e+00     3.482259e-02
 * time: 7.390975952148438e-5
     1    -6.963174e+00     1.854197e-02
 * time: 0.7571229934692383
     2    -7.449229e+00     8.711296e-03
 * time: 1.0952630043029785
     3    -7.533046e+00     7.885645e-03
 * time: 1.574234962463379
     4    -7.550536e+00     4.197516e-03
 * time: 1.9220640659332275
     5    -7.555661e+00     4.174901e-03
 * time: 2.44358491897583
     6    -7.558786e+00     2.405364e-03
 * time: 2.9710330963134766
     7    -7.559523e+00     1.402889e-03
 * time: 3.3233070373535156
     8    -7.559876e+00     5.139698e-04
 * time: 3.786884069442749
     9    -7.559892e+00     2.825655e-04
 * time: 4.129610061645508
    10    -7.559918e+00     1.110918e-04
 * time: 4.7332069873809814
</code></pre><p>Observe that the Sinkhorn divergence results in <span>$\nu$</span> that matches <span>$\mu$</span> quite well, while entropy-regularised transport is biased to producing <span>$\nu$</span> that seems to concentrate around the mean of each Gaussian component.</p><pre><code class="language-julia hljs">plot(plt1, plt2)</code></pre><p><img src="../1245948687.svg" alt/></p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../basic/">« Basics</a><a class="docs-footer-nextpage" href="../nmf/">Wasserstein non-negative matrix factorisation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Tuesday 2 May 2023 01:02">Tuesday 2 May 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
