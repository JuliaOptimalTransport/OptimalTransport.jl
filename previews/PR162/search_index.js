var documenterSearchIndex = {"docs":
[{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"EditURL = \"https://github.com/JuliaOptimalTransport/OptimalTransport.jl/blob/master/examples/empirical_sinkhorn_div/script.jl\"","category":"page"},{"location":"examples/empirical_sinkhorn_div/#Sinkhorn-divergences","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"","category":"section"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"(Image: )","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"In this tutorial we provide a minimal example for using the Sinkhorn divergence as a loss function [FSV+19] on empirical distributions. [FSV+19]: Feydy, Jean, et al. \"Interpolating between optimal transport and MMD using Sinkhorn divergences.\" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"While entropy-regularised optimal transport operatornameOT_varepsilon(cdot cdot) is commonly used as a loss function, it suffers from a problem of bias: namely that nu mapsto operatornameOT_varepsilon(mu nu) is not minimised at nu = mu.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"A fix to this problem is proposed by Genevay et al [GPC18] and subsequently Feydy et al. [FSV+19], which introduce the Sinkhorn divergence between two measures mu and nu, defined as","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"operatornameS_varepsilon(mu nu) = operatornameOT_varepsilon(mu nu) - frac12 operatornameOT_varepsilon(mu mu) - frac12 operatornameOT_varepsilon(nu nu)","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"In the above, we have followed the convention taken by Feydy et al. and included the entropic regularisation in the definition of operatornameOT_varepsilon. [GPC18]: Aude Genevay, Gabriel Peyré, Marco Cuturi, Learning Generative Models with Sinkhorn Divergences, Proceedings of the Twenty-First International Conference on Artficial Intelligence and Statistics, (AISTATS) 21, 2018 [FSV+19]: Feydy, Jean, et al. \"Interpolating between optimal transport and MMD using Sinkhorn divergences.\" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Like the Sinkhorn loss, the Sinkhorn divergence is smooth and convex in both of its arguments. However, the Sinkhorn divergence is unbiased – i.e. S_varepsilon(mu nu) = 0 iff mu = nu.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Unlike previous examples, here we demonstrate a learning problem similar to Figure 1 of Feydy et al. over empirical measures, i.e. measures that have the form mu = frac1N sum_i = 1^N delta_x_i where delta_x is the Dirac delta function at x.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"We first load packages.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"using OptimalTransport\nusing ReverseDiff\nusing Distributions\nusing LinearAlgebra\nusing Distances\nusing Plots\nusing Logging\nusing Optim","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"As a ground truth distribution, we set rho to be a Gaussian mixture model with k = 3 components, equally spaced around a circle, and sample an empirical distribution of size N, mu sim rho.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"k = 3\nd = 2\nθ = π * range(0, 2(1 - 1 / k); length=k)\nμ = 2 * hcat(sin.(θ), cos.(θ))\nρ = MixtureModel(MvNormal[MvNormal(x, 0.25 * I) for x in eachrow(μ)])\nN = 100\nμ_spt = rand(ρ, N)'\nscatter(μ_spt[:, 1], μ_spt[:, 2]; markeralpha=0.25, title=raw\"$\\mu$\")","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"(Image: )","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Now, suppose we want to approximate mu with another empirical distribution nu, i.e. we want to minimise nu mapsto operatornameS_varepsilon(mu nu) over possible empirical distributions nu. In this case we have M particles in nu, which we initialise following a Gaussian distribution.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"M = 100\nν_spt = rand(M, d);","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Assign uniform weights to the Diracs in each empirical distribution.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"μ = fill(1 / N, N)\nν = fill(1 / M, M);","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Since mu is fixed, we pre-compute the cost matrix C_mu.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"C_μ = pairwise(SqEuclidean(), μ_spt');","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Define the loss function to minimise, where x specifies the locations of the Diracs in nu.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"We will be using ReverseDiff with a precompiled tape. For this reason, we need the Sinkhorn algorithm to perform a fixed number of (e.g. 50) iterations. Currently, this can be achieved by setting maxiter = 50 and atol = rtol = 0 in calls to sinkhorn and sinkhorn_divergence.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"function loss(x, ε)\n    C_μν = pairwise(SqEuclidean(), μ_spt', x')\n    C_ν = pairwise(SqEuclidean(), x')\n    return sinkhorn_divergence(\n        μ, ν, C_μν, C_μ, C_ν, ε; maxiter=50, atol=rtol = 0, regularization=true\n    )\nend","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"loss (generic function with 1 method)","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Set entropy regularisation parameter","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"ε = 1.0;","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Use ReverseDiff with a precompiled tape and Optim.jl to minimise nu mapsto operatornameS_varepsilon(mu nu). Note that this is problem is not convex, so we find a local minimium.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"const loss_tape = ReverseDiff.GradientTape(x -> loss(x, ε), ν_spt)\nconst compiled_loss_tape = ReverseDiff.compile(loss_tape)\nopt = with_logger(SimpleLogger(stderr, Logging.Error)) do\n    optimize(\n        x -> loss(x, ε),\n        (∇, x) -> ReverseDiff.gradient!(∇, compiled_loss_tape, x),\n        ν_spt,\n        GradientDescent(),\n        Optim.Options(; iterations=10, g_tol=1e-6, show_trace=true),\n    )\nend\nν_opt = Optim.minimizer(opt)\nplt1 = scatter(μ_spt[:, 1], μ_spt[:, 2]; markeralpha=0.25, title=\"Sinkhorn divergence\")\nscatter!(plt1, ν_opt[:, 1], ν_opt[:, 2]);","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Iter     Function value   Gradient norm \n     0     3.181187e+00     3.517824e-02\n * time: 0.08664298057556152\n     1     1.967282e-01     1.542582e-02\n * time: 5.353132963180542\n     2     2.398086e-02     6.949531e-03\n * time: 6.321340799331665\n     3     6.981686e-03     2.802920e-03\n * time: 7.652906894683838\n     4     1.431873e-03     8.131897e-04\n * time: 8.54528284072876\n     5     7.998772e-04     6.238553e-04\n * time: 9.936904907226562\n     6     6.263259e-04     3.546573e-04\n * time: 11.30807900428772\n     7     5.360041e-04     2.802750e-04\n * time: 12.121232986450195\n     8     4.761683e-04     2.115341e-04\n * time: 13.526302814483643\n     9     4.321897e-04     1.815395e-04\n * time: 14.485275983810425\n    10     3.978220e-04     1.689752e-04\n * time: 15.924569845199585\n","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"For comparison, let us do the same computation again, but this time we want to minimise nu mapsto operatornameOT_varepsilon(mu nu).","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"function loss_biased(x, ε)\n    C_μν = pairwise(SqEuclidean(), μ_spt', x')\n    return sinkhorn2(μ, ν, C_μν, ε; maxiter=50, atol=rtol = 0, regularization=true)\nend\nconst loss_biased_tape = ReverseDiff.GradientTape(x -> loss_biased(x, ε), ν_spt)\nconst compiled_loss_biased_tape = ReverseDiff.compile(loss_biased_tape)\nopt_biased = with_logger(SimpleLogger(stderr, Logging.Error)) do\n    optimize(\n        x -> loss_biased(x, ε),\n        (∇, x) -> ReverseDiff.gradient!(∇, compiled_loss_biased_tape, x),\n        ν_spt,\n        GradientDescent(),\n        Optim.Options(; iterations=10, g_tol=1e-6, show_trace=true),\n    )\nend\nν_opt_biased = Optim.minimizer(opt_biased)\nplt2 = scatter(μ_spt[:, 1], μ_spt[:, 2]; markeralpha=0.25, title=\"Sinkhorn loss\")\nscatter!(plt2, ν_opt_biased[:, 1], ν_opt_biased[:, 2]);","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Iter     Function value   Gradient norm \n     0    -4.993879e+00     2.886518e-02\n * time: 0.0001239776611328125\n     1    -7.203191e+00     1.788522e-02\n * time: 1.6640560626983643\n     2    -7.572464e+00     1.309952e-02\n * time: 2.352059841156006\n     3    -7.672704e+00     1.236911e-02\n * time: 3.2991108894348145\n     4    -7.697164e+00     2.841203e-03\n * time: 3.936042070388794\n     5    -7.698474e+00     1.126256e-03\n * time: 4.5104148387908936\n     6    -7.698746e+00     5.812328e-04\n * time: 5.53758692741394\n     7    -7.698741e+00     2.401605e-04\n * time: 6.069823980331421\n     8    -7.698766e+00     1.253177e-04\n * time: 6.876394987106323\n     9    -7.698759e+00     5.259302e-05\n * time: 7.326931953430176\n    10    -7.698763e+00     2.761157e-05\n * time: 7.9206788539886475\n","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"Observe that the Sinkhorn divergence results in nu that matches mu quite well, while entropy-regularised transport is biased to producing nu that seems to concentrate around the mean of each Gaussian component.","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"plot(plt1, plt2)","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"(Image: )","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"","category":"page"},{"location":"examples/empirical_sinkhorn_div/","page":"Sinkhorn divergences","title":"Sinkhorn divergences","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"EditURL = \"https://github.com/JuliaOptimalTransport/OptimalTransport.jl/blob/master/examples/nmf/script.jl\"","category":"page"},{"location":"examples/nmf/#Wasserstein-non-negative-matrix-factorisation","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"","category":"section"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"(Image: )","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"In this example, we implement Wasserstein non-negative matrix factorisation (W-NMF) following the paper [RCP16] by Rolet et al.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"[RCP16]: Rolet, Antoine, Marco Cuturi, and Gabriel Peyré. \"Fast dictionary learning with a smoothed Wasserstein loss.\" Artificial Intelligence and Statistics. PMLR, 2016.","category":"page"},{"location":"examples/nmf/#Introduction","page":"Wasserstein non-negative matrix factorisation","title":"Introduction","text":"","category":"section"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Matrix decomposition is a classical problem in machine learning. Given a m times n matrix X representing n repeated m-dimensional observations, one may seek matrices D Lambda of appropriate dimensions such that","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"X approx D Lambda","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"For a target rank k  min  m n , i.e. D in mathbbR^m times k Lambda in mathbbR^k times n, this problem can be thought of as seeking a low-dimensional linear representation D Lambda of the potentially high-dimensional dataset X.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Lee and Seung [LS99] observed that the data matrix X is non-negative in many practical applications, and that naturally one may want the factor matrices D Lambda to be also non-negative.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"[LS99]: Lee, Daniel D., and H. Sebastian Seung. \"Learning the parts of objects by non-negative matrix factorization.\" Nature 401.6755 (1999): 788-791.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"For a given m times n matrix X ge 0, finding the factor matrices D in mathbbR^m times k Lambda in mathbbR^k times n such that X approx D Lambda with D ge 0 Lambda ge 0 is known as the rank-k non-negative matrix factorization (NMF) problem. Typically, such an approximate representation is sought by solving a minimisation problem","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"min_D in mathbbR^m times k Lambda in mathbbR^k times n D ge 0 Lambda ge 0 Phi(X D Lambda)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"where (X Y) mapsto Psi(X Y) is a loss function defined on matrices. Commonly used choices of Phi include the squared Frobenius loss Phi(X Y) =  X - Y _F^2 and Kullback-Leibler divergence Phi(X Y) = sum_ij X_ij log(X_ijY_ij).","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Both these loss functions (and many other common choices) decompose elementwise in their arguments, that is, they can be written as Phi(X Y) = sum_ij f(X_ij Y_ij) for some function f acting on scalars.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Rolet et al. note that pointwise loss functions cannot account for spatial correlations in datasets with underlying geometry, and propose to use entropy-regularised optimal transport as a loss function that is sensitive to the spatial arrangement of the data. They argue that for datasets such as images, optimal transport is a more natural choice of loss function, and that it achieves superior performance.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"In particular, suppose that the columns of X encode image data, and that C in mathbbR^m times m encodes the squared Euclidean distances on the imaging domain. The problem that Rolet et al. pose is","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"min_D in mathbbR^m times k_ge 0 Lambda in mathbbR^k times n_ge 0 sum_i = 1^n operatornameOT_varepsilon(X_i (DLambda)_i) + rho_1 E(D) + rho_2 E(Lambda)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"where operatornameOT_varepsilon(alpha beta) is the entropy-regularised optimal transport loss between two probability distributions alpha beta for a cost C, and E is an entropy barrier function (a smooth approximation to the non-negativity constraint),","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"E(A) = sum_ij (A_ij log(A_ij) - 1)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"The parameters rho_1 rho_2 control how \"sharp\" the non-negativity constraints are. As rho_1 rho_2 to 0, the smoothed constraint approaches the hard non-negativity constraint. Finally, varepsilon controls the regularisation level for the optimal transport loss.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"This example shows how this method can be implemented using functions from the Dual submodule of OptimalTransport.jl.","category":"page"},{"location":"examples/nmf/#Set-up-prerequisites","page":"Wasserstein non-negative matrix factorisation","title":"Set up prerequisites","text":"","category":"section"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Load packages that will be required later.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"using OptimalTransport\nimport OptimalTransport.Dual: Dual\nusing MLDatasets: MLDatasets\nusing StatsBase\nusing Plots;\ndefault(; palette=:Set1_3)\nusing LogExpFunctions\nusing NNlib: NNlib\nusing LinearAlgebra\nusing Distances\nusing Base.Iterators\nusing NMF\nusing Optim","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Define simplex_norm!, which normalises x to sum to 1 along dims.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"function simplex_norm!(x; dims=1)\n    return x .= x ./ sum(x; dims=dims)\nend","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"simplex_norm! (generic function with 1 method)","category":"page"},{"location":"examples/nmf/#Implementation","page":"Wasserstein non-negative matrix factorisation","title":"Implementation","text":"","category":"section"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We now implement Wasserstein-NMF. Rolet et al. split the original non-convex problem into a pair of convex problems, one for D and one for Lambda.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"beginaligned\nmin_D in mathbbR^m times k sum_i = 1^n operatornameOT_varepsilon(X_i (DLambda)_i) + rho_2 E(D) \nmin_Lambda in mathbbR^k times n sum_i = 1^n operatornameOT_varepsilon(X_i (DLambda)_i) + rho_1 E(Lambda)\nendaligned","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"For each of these problems, the dual problem can be derived (see Section 3.3 of Rolet et al. for details on how this is done). These turn out to be","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"beginaligned\nmin_G in mathbbR^m times n sum_i = 1^n operatornameOT_varepsilon^*(X_i G_i) + rho_2 sum_i = 1^n E^*(-(G Lambda^top)_i  rho_2) \nmin_G in mathbbR^m times n sum_i = 1^n operatornameOT_varepsilon^*(X_i G_i) + rho_1 sum_i = 1^n E^*(-(D^top G)_i  rho_1)\nendaligned","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"The semi-dual of entropy-regularised optimal transport loss, operatornameOT_varepsilon, is implemented as Dual.ot_entropic_semidual and its gradient can be computed by Dual.ot_entropic_semidual_grad. E^* turns out to be logsumexp, for which we implement a wrapper function:","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"function E_star(x; dims=1)\n    return logsumexp(x; dims=dims)\nend;","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"The gradient of logsumexp is softmax, so we define also its gradient:","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"function E_star_grad(x; dims=1)\n    return NNlib.softmax(x; dims=1)\nend;","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Thus, for each problem we may define the dual objective and its gradient. We note that ot_entropic_semidual(_grad) automatically broadcasts along columns of its input. There is therefore no need to make multiple calls to the function, thus allowing for more efficient evaluation.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"function dual_obj_weights(X, K, ε, D, G, ρ1)\n    return sum(Dual.ot_entropic_semidual(X, G, ε, K)) + ρ1 * sum(E_star(-D' * G / ρ1))\nend\nfunction dual_obj_weights_grad!(∇, X, K, ε, D, G, ρ1)\n    return ∇ .= Dual.ot_entropic_semidual_grad(X, G, ε, K) - D * E_star_grad(-D' * G / ρ1)\nend\nfunction dual_obj_dict(X, K, ε, Λ, G, ρ2)\n    return sum(Dual.ot_entropic_semidual(X, G, ε, K)) + ρ2 * sum(E_star(-G * Λ' / ρ2))\nend\nfunction dual_obj_dict_grad!(∇, X, K, ε, Λ, G, ρ2)\n    return ∇ .= Dual.ot_entropic_semidual_grad(X, G, ε, K) - E_star_grad(-G * Λ' / ρ2) * Λ\nend;","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"The only remaining part of Wasserstein-NMF to implement is the conversion at optimality from the dual variable G to the primal variables D Lambda. From the results of Theorems 3 and 4 in Rolet et al., we have for Lambda:","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Lambda_i = operatornamesoftmax((-D^top G)_i  rho_1)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"and we have for D:","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"D_i = operatornamesoftmax((-G Lambda^top)_i  rho_2)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"function getprimal_weights(D, G, ρ1)\n    return NNlib.softmax(-D' * G / ρ1; dims=1)\nend\nfunction getprimal_dict(Λ, G, ρ2)\n    return NNlib.softmax(-G * Λ' / ρ2; dims=1)\nend;","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We can now implement functions solve_weights and solve_dict that solve the respective dual problems for the next iterates of Λ and D.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"function solve_weights(X, K, ε, D, ρ1; alg, options)\n    opt = optimize(\n        g -> dual_obj_weights(X, K, ε, D, g, ρ1),\n        (∇, g) -> dual_obj_weights_grad!(∇, X, K, ε, D, g, ρ1),\n        zero.(X),\n        alg,\n        options,\n    )\n    return getprimal_weights(D, Optim.minimizer(opt), ρ1)\nend\nfunction solve_dict(X, K, ε, Λ, ρ2; alg, options)\n    opt = optimize(\n        g -> dual_obj_dict(X, K, ε, Λ, g, ρ2),\n        (∇, g) -> dual_obj_dict_grad!(∇, X, K, ε, Λ, g, ρ2),\n        zero.(X),\n        alg,\n        options,\n    )\n    return getprimal_dict(Λ, Optim.minimizer(opt), ρ2)\nend;","category":"page"},{"location":"examples/nmf/#Example:-noisy-univariate-Gaussians","page":"Wasserstein non-negative matrix factorisation","title":"Example: noisy univariate Gaussians","text":"","category":"section"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We set up each observation as a mixture of 3 Gaussians with means sampled from N(6, σ), N(0, σ), N(-6, σ) respectively, and mixture weights sampled uniformly from [0, 1]. The resulting mixture model is normalised to sum to 1 on the discrete domain coord.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"f(x, μ, σ) = exp.(-(x .- μ) .^ 2)\ncoord = range(-12, 12; length=100)\nN = 100\nσ = 1\nX = hcat(\n    [\n        rand() * f(coord, σ * randn() + 6, 1) +\n        rand() * f(coord, σ * randn(), 1) +\n        rand() * f(coord, σ * randn() - 6, 1) for _ in 1:N\n    ]...,\n)\nX = simplex_norm!(X);","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We visualise the  observations.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"plot(coord, X; alpha=0.1, color=:blue, title=\"Input data\", legend=nothing)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"(Image: )","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We can apply NMF with a squared Frobenius loss using the NMF.jl package. We seek k = 3 components.  This performs poorly, since the pointwise nature of the loss function cannot handle the translational noise in the data.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"k = 3\nresult = nnmf(X, k; alg=:multmse)\nplot(coord, result.W; title=\"NMF with Frobenius loss\", palette=:Set1_3)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"(Image: )","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We can now set up a cost matrix corresponding to the domain coord.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"C = pairwise(SqEuclidean(), coord)\nC = C / mean(C);","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Specify parameters","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"ε = 0.025\nρ1, ρ2 = (5e-2, 5e-2);","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Compute Gibbs kernel","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"K = exp.(-C / ε);","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Now we use a random initialisation, where columns of D and Λ are normalised to sum to 1.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"D = rand(size(X, 1), k) # dictionary\nsimplex_norm!(D; dims=1) # norm columnwise\nΛ = rand(k, size(X, 2)) # weights\nsimplex_norm!(Λ; dims=1); # norm rowwise","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We now run 10 iterations of Wasserstein-NMF.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"n_iter = 10\nfor iter in 1:n_iter\n    @info \"Wasserstein-NMF: iteration $iter\"\n    D .= solve_dict(\n        X,\n        K,\n        ε,\n        Λ,\n        ρ2;\n        alg=LBFGS(),\n        options=Optim.Options(;\n            iterations=250, g_tol=1e-4, show_trace=false, show_every=10\n        ),\n    )\n    Λ .= solve_weights(\n        X,\n        K,\n        ε,\n        D,\n        ρ1;\n        alg=LBFGS(),\n        options=Optim.Options(;\n            iterations=250, g_tol=1e-4, show_trace=false, show_every=10\n        ),\n    )\nend","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"[ Info: Wasserstein-NMF: iteration 1\n[ Info: Wasserstein-NMF: iteration 2\n[ Info: Wasserstein-NMF: iteration 3\n[ Info: Wasserstein-NMF: iteration 4\n[ Info: Wasserstein-NMF: iteration 5\n[ Info: Wasserstein-NMF: iteration 6\n[ Info: Wasserstein-NMF: iteration 7\n[ Info: Wasserstein-NMF: iteration 8\n[ Info: Wasserstein-NMF: iteration 9\n[ Info: Wasserstein-NMF: iteration 10\n","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We observe that Wasserstein-NMF learns atoms (columns of D) corresponding to the three Gaussians used to generate the input data.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"plot(coord, D; title=\"NMF with Wasserstein loss\", palette=:Set1_3)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"(Image: )","category":"page"},{"location":"examples/nmf/#Example:-image-data-(MNIST)","page":"Wasserstein non-negative matrix factorisation","title":"Example: image data (MNIST)","text":"","category":"section"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Here we will download MNIST dataset using MLDatasets.jl and downscale each image to 14x14 to allow for faster runtime (since we are running on CPU).","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"sizex, sizey = 28, 28\nfactor = 2 # downscale factor\nΣ = hcat([sum(I(sizex)[:, i:(i + factor - 1)]; dims=2) for i in 1:factor:sizex]...)\nsizex, sizey = sizex ÷ factor, sizey ÷ factor\nN = 256\nx, y = MLDatasets.MNIST.traindata(Float64, sample(1:60_000, N; replace=false))\nx = permutedims(x, (2, 1, 3))\nx = cat([Σ' * x[:, :, i] * Σ for i in 1:N]...; dims=3)\nX = simplex_norm!(reshape(x, (sizex * sizey, :)));","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"The columns of X now correspond to images \"flattened\" as vectors. We can preview a few images.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"M = 25\nplot(\n    [\n        heatmap(\n            reshape(X[:, i], sizex, sizey);\n            legend=:none,\n            axis=nothing,\n            showaxis=false,\n            aspect_ratio=:equal,\n            c=:Blues,\n            yflip=true,\n        ) for i in 1:M\n    ]...;\n    layout=(5, M ÷ 5),\n    plot_title=\"Input images\",\n)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"(Image: )","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Now we can set up coord, cost matrix C, and specify parameters.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"coord = reshape(collect(product(1:sizex, 1:sizey)), :)\nC = pairwise(SqEuclidean(), coord)\nC = C / mean(C);\nε = 0.0025\nρ1, ρ2 = (5e-3, 5e-3);","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We compute the Gibbs kernel from C:","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"K = exp.(-C / ε);","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Let us aim to learn k = 25 atoms.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"k = 25;","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Initialise again randomly","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"D = rand(size(X, 1), k) # dictionary\nsimplex_norm!(D; dims=1) # norm columnwise\nΛ = rand(k, size(X, 2)) # weights\nsimplex_norm!(Λ; dims=1); # norm rowwise","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We now run 15 iterations of Wasserstein-NMF.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"n_iter = 15\nfor iter in 1:n_iter\n    @info \"Wasserstein-NMF: iteration $iter\"\n    D .= solve_dict(\n        X,\n        K,\n        ε,\n        Λ,\n        ρ2;\n        alg=LBFGS(),\n        options=Optim.Options(;\n            iterations=250, g_tol=1e-4, show_trace=false, show_every=10\n        ),\n    )\n    Λ .= solve_weights(\n        X,\n        K,\n        ε,\n        D,\n        ρ1;\n        alg=LBFGS(),\n        options=Optim.Options(;\n            iterations=250, g_tol=1e-4, show_trace=false, show_every=10\n        ),\n    )\nend","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"[ Info: Wasserstein-NMF: iteration 1\n[ Info: Wasserstein-NMF: iteration 2\n[ Info: Wasserstein-NMF: iteration 3\n[ Info: Wasserstein-NMF: iteration 4\n[ Info: Wasserstein-NMF: iteration 5\n[ Info: Wasserstein-NMF: iteration 6\n[ Info: Wasserstein-NMF: iteration 7\n[ Info: Wasserstein-NMF: iteration 8\n[ Info: Wasserstein-NMF: iteration 9\n[ Info: Wasserstein-NMF: iteration 10\n[ Info: Wasserstein-NMF: iteration 11\n[ Info: Wasserstein-NMF: iteration 12\n[ Info: Wasserstein-NMF: iteration 13\n[ Info: Wasserstein-NMF: iteration 14\n[ Info: Wasserstein-NMF: iteration 15\n","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"We can inspect the atoms learned (columns of D):","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"plot(\n    [\n        heatmap(\n            reshape(D[:, i], sizex, sizey);\n            legend=:none,\n            axis=nothing,\n            showaxis=false,\n            aspect_ratio=:equal,\n            c=:Blues,\n            yflip=true,\n        ) for i in 1:k\n    ]...;\n    layout=(5, k ÷ 5),\n    plot_title=\"Learned atoms\",\n)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"(Image: )","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"Finally, we can look at the images constructed by the low-rank approximation DΛ and compare to the original images that we previewed earlier.","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"X_hat = D * Λ\nplot(\n    [\n        heatmap(\n            reshape(X_hat[:, i], sizex, sizey);\n            legend=:none,\n            axis=nothing,\n            showaxis=false,\n            aspect_ratio=:equal,\n            c=:Blues,\n            yflip=true,\n        ) for i in 1:M\n    ]...;\n    layout=(5, M ÷ 5),\n    plot_title=\"Low rank approximation\",\n)","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"(Image: )","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"","category":"page"},{"location":"examples/nmf/","page":"Wasserstein non-negative matrix factorisation","title":"Wasserstein non-negative matrix factorisation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"EditURL = \"https://github.com/JuliaOptimalTransport/OptimalTransport.jl/blob/master/examples/variational/script.jl\"","category":"page"},{"location":"examples/variational/#Variational-problems","page":"Variational problems","title":"Variational problems","text":"","category":"section"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"(Image: )","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"In this example, we will numerically simulate an entropy-regularised Wasserstein gradient flow approximating the Fokker-Planck and porous medium equations.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"The connection between Wasserstein gradient flows and (non)-linear PDEs is due to Jordan, Kinderlehrer and Otto [JKO98], and an easy-to-read overview of the topic is provided in Section 9.3 [PC19]","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"[JKO98]: Jordan, Richard, David Kinderlehrer, and Felix Otto. \"The variational formulation of the Fokker–Planck equation.\" SIAM journal on mathematical analysis 29.1 (1998): 1-17.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"[PC19]: Peyré, Gabriel, and Marco Cuturi. \"Computational optimal transport: With applications to data science.\" Foundations and Trends® in Machine Learning 11.5-6 (2019): 355-607.","category":"page"},{"location":"examples/variational/#Fokker-Planck-equation-as-a-W_2-gradient-flow","page":"Variational problems","title":"Fokker-Planck equation as a W_2 gradient flow","text":"","category":"section"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"For a potential function Psi and noise level sigma^2, the Fokker-Planck equation (FPE) is","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"partial_t rho_t = nabla cdot (rho_t nabla Psi) + fracsigma^22 Delta rho_t","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"and we take no-flux (Neumann) boundary conditions.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"This describes the evolution of a massless particle undergoing both diffusion (with diffusivity sigma^2) and drift (along potential Psi) according to the stochastic differential equation","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"dX_t = -nabla Psi(X_t) dt + sigma dB_t","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"The result of Jordan, Kinderlehrer and Otto (commonly referred to as the JKO theorem) states that rho_t evolves following the 2-Wasserstein gradient flow of the Gibbs free energy functional","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"  F(rho) = int Psi drho + int log(rho) drho","category":"page"},{"location":"examples/variational/#Implicit-schemes-for-gradient-flows","page":"Variational problems","title":"Implicit schemes for gradient flows","text":"","category":"section"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"In an Euclidean space, the gradient flow of a functional F is simply the solution of an ordinary differential equation","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":" dfracdx(t)dt = -nabla F(x(t))","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Of course, there is a requirement that F is smooth. A more general formulation of a gradient flow that allows F to be non-smooth is the implicit scheme","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"  x_t+tau = operatornameargmin_x frac12  x - x_t _2^2 + tau F(x)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"As the timestep tau shrinks, x_t becomes a better and better approximation to the gradient flow of F.","category":"page"},{"location":"examples/variational/#Wasserstein-gradient-flow","page":"Variational problems","title":"Wasserstein gradient flow","text":"","category":"section"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"In the context of the JKO theorem, we seek rho_t that is the gradient flow of F with respect to the 2-Wasserstein distance. This can be achieved by choosing the W_2 metric in the implicit step:","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"  rho_t + tau = operatornameargmin_rho d_W_2^2(rho_t rho) + tau F(rho)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Finally, a numerical scheme for computing this gradient flow can be developed by using the entropic regularisation of optimal transport on a discretised domain","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"  rho_t + tau = operatornameargmin_rho operatornameOT_varepsilon(rho_t rho) + tau F(rho)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"where","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"  operatornameOT_varepsilon(alpha beta) = min_gamma in Pi(alpha beta) sum_ij frac12  x_i - x_j _2^2 gamma_ij + varepsilon sum_i j gamma_ij log(gamma_ij)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Each step of this problem is a minimisation problem with respect to rho. Since we use entropic optimal transport which is differentiable, this can be solved using gradient-based methods.","category":"page"},{"location":"examples/variational/#Problem-setup","page":"Variational problems","title":"Problem setup","text":"","category":"section"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"using OptimalTransport\nusing Distances\nusing LogExpFunctions\nusing Optim\nusing Plots\nusing StatsBase\nusing ReverseDiff\n\nusing LinearAlgebra\nusing Logging","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Here, we set up the computational domain that we work on - we discretize the interval -1 1. The natural boundary conditions to use will be Neumann (zero flux), see e.g. [Santam2017]","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"[Santam2017]: Santambrogio, Filippo. \"{Euclidean, metric, and Wasserstein} gradient flows: an overview.\" Bulletin of Mathematical Sciences 7.1 (2017): 87-154.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"support = range(-1, 1; length=64)\nC = pairwise(SqEuclidean(), support');","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Now we set up various functionals that we will use.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"We define the generalised entropy (Equation (4.4) of [Peyre2015]) as follows. For m = 1 this is just the \"regular\" entropy, and m = 2 this is squared L_2.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"[Peyre2015]: Peyré, Gabriel. \"Entropic approximation of Wasserstein gradient flows.\" SIAM Journal on Imaging Sciences 8.4 (2015): 2323-2351.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"function E(ρ; m=1)\n    if m == 1\n        return sum(xlogx.(ρ)) - sum(ρ)\n    elseif m > 1\n        return dot(ρ, @. (ρ^(m - 1) - m) / (m - 1))\n    end\nend;","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Now define psi(x) to be a potential energy function that has two potential wells at x =  05.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"ψ(x) = 10 * (x - 0.5)^2 * (x + 0.5)^2;\nplot(support, ψ.(support); color=\"black\", label=\"Scalar potential\")","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"(Image: )","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Having defined psi, this induces a potential energy functional Psi on probability distributions rho:","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"   Psi(rho) = int psi(x) rho(x) dx = langle psi rho rangle ","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Ψ = ψ.(support);","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Define the time step tau and entropic regularisation level varepsilon, and form the associated Gibbs kernel K = e^-Cvarepsilon.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"τ = 0.05\nε = 0.01\nK = @. exp(-C / ε);","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"We define the (non-smooth) initial condition rho_0 in terms of step functions.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"H(x) = x > 0\nρ0 = @. H(support + 0.25) - H(support - 0.25)\nρ0 = ρ0 / sum(ρ0)\nplot(support, ρ0; label=\"Initial condition ρ0\", color=\"blue\")","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"(Image: )","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"G_fpe is the objective function for the implicit step scheme","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"G_mathrmfpe(rho) = operatornameOT_varepsilon(rho_t rho) + tau F(rho)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"and we seek to minimise in rho.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"function G_fpe(ρ, ρ0, τ, ε, C)\n    return sinkhorn2(ρ, ρ0, C, ε; regularization=true, maxiter=250) + τ * (dot(Ψ, ρ) + E(ρ))\nend;","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"step solves the implicit step problem to produce rho_t + tau from rho_t.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"function step(ρ0, τ, ε, C, G)\n    # only print error messages\n    obj = u -> G(softmax(u), ρ0, τ, ε, C)\n    opt = with_logger(SimpleLogger(stderr, Logging.Error)) do\n        optimize(\n            obj,\n            ones(size(ρ0)),\n            LBFGS(),\n            Optim.Options(; iterations=50, g_tol=1e-6);\n            autodiff=:forward,\n        )\n    end\n    return softmax(Optim.minimizer(opt))\nend","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"step (generic function with 1 method)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Now we simulate N = 10 iterates of the gradient flow and plot the result.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"N = 10\nρ = similar(ρ0, size(ρ0, 1), N)\nρ[:, 1] = ρ0\nfor i in 2:N\n    @info i\n    ρ[:, i] = step(ρ[:, i - 1], τ, ε, C, G_fpe)\nend\ncolors = range(colorant\"red\"; stop=colorant\"blue\", length=N)\nplot(\n    support,\n    ρ;\n    title=raw\"$F(\\rho) = \\langle \\psi, \\rho \\rangle + \\langle \\rho, \\log(\\rho) \\rangle$\",\n    palette=colors,\n    legend=nothing,\n)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"(Image: )","category":"page"},{"location":"examples/variational/#Porous-medium-equation","page":"Variational problems","title":"Porous medium equation","text":"","category":"section"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"The porous medium equation (PME) is the nonlinear PDE","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"partial_t rho = nabla cdot (rho nabla Psi) + Delta rho^m","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"again with Neumann boundary conditions. The value of m in the PME corresponds to picking m in the generalised entropy functional. Now, we will solve the PME with m = 2 as a Wasserstein gradient flow.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"function G_pme(ρ, ρ0, τ, ε, C)\n    return sinkhorn2(ρ, ρ0, C, ε; regularization=true, maxiter=250) +\n           τ * (dot(Ψ, ρ) + E(ρ; m=2))\nend;","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"set up as previously","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"N = 10\nρ = similar(ρ0, size(ρ0, 1), N)\nρ[:, 1] = ρ0\nfor i in 2:N\n    ρ[:, i] = step(ρ[:, i - 1], τ, ε, C, G_pme)\nend\nplot(\n    support,\n    ρ;\n    title=raw\"$F(\\rho) = \\langle \\psi, \\rho \\rangle + \\langle \\rho, \\rho - 1\\rangle$\",\n    palette=colors,\n    legend=nothing,\n)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"(Image: )","category":"page"},{"location":"examples/variational/#Exploiting-duality","page":"Variational problems","title":"Exploiting duality","text":"","category":"section"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"The previous examples solved the minimisation problem for the implicit gradient flow step directly, involving automatic differentiation through the Sinkhorn iterations used to compute operatornameOT_varepsilon(rho_t rho) each time a gradient needs to be evaluated. While this is straightforward to implement, it is computationally costly. An alternative approach for convex variational problems is to proceed via the dual problem. The benefit of proceeding via the dual problem is that the part of the dual minimisation problem corresponding to the (entropy-regularised) optimal transport loss is typically available in closed form. This is in contrast to the primal problem, where evaluation of the objective and its gradients requires potentially many Sinkhorn iterations.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Consider a general convex and unconstrained problem. Under (usually satisfied) conditions for strong duality to hold, we have","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"beginaligned\nmin_rho operatornameOT_varepsilon(rho_0 rho) + mathcalF(rho)  \n= min_rho sup_uleftlangle rho u rangle - operatornameOT^*_varepsilon(rho_0 u)right + mathcalF(rho)  \n= sup_u min_rho langle rho u rangle - operatornameOT^*_varepsilon(rho_0 u) + mathcalF(rho) \n= sup_u - operatornameOT^*_varepsilon(rho_0 u) + min_rho langle rho u rangle + mathcalF(rho) \n= sup_u - operatornameOT^*_varepsilon(rho_0 u) - sup_rho langle rho -u rangle - mathcalF(rho) \n= sup_u - operatornameOT^*_varepsilon(rho_0 u) - mathcalF^*(-u)\nendaligned","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Thus, the dual problem is","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"min_u operatornameOT^*_varepsilon(rho_0 u) + mathcalF^*(-u)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"The upshot here is that u mapsto operatornameOT^*_varepsilon(rho_0 u) and its gradient is available in closed form. This is a known result in the literature [CP18].","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"[CP18]: Cuturi, Marco, and Gabriel Peyré. “Semi-Dual Regularized Optimal Transport.” ArXiv: Learning, 2018.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"The formulas we state below are lifted from statements in [Z21].","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"[Z21]: Zhang, Stephen Y. “A Unified Framework for Non-Negative Matrix and Tensor Factorisations with a Smoothed Wasserstein Loss.” ArXiv: Machine Learning, 2021.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"beginaligned\noperatornameOT^*_varepsilon(rho_0 u) = -varepsilon leftlangle rho_0 logleft( dfracrho_0K e^uvarepsilon right) - 1rightrangle \nnabla_u operatornameOT^*_varepsilon(rho_0 u) = K^top left( dfracrho_0K e^uvarepsilon right) odot e^uvarepsilon\nendaligned","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"At optimality, we can recover the primal optimal point rho^star from the dual optimal point u^star following the formula","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"rho^star = e^u^starvarepsilon odot K^top dfracrho_0K e^u^starvarepsilon","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"When mathcalF^*(cdot) is also available in closed form (this is not always the case), the dual problem has a closed form objective and can generally be solved much more efficiently than the primal problem.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"In the setting of the Fokker-Planck and porous medium equations, the function mathcalF can be identified with","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"mathcalF(rho) = tau left langle psi rho rangle + E_m(rho) right","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"A straightforward computation shows that","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"mathcalF^*(u) = tau E_m^*left( fracutau-psi right)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"where","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"    E_m^*(u) = begincases\n    langle e^u mathbf1 rangle  m = 1 \n    sum_i left left( u_i + fracmm-1 right) left( fracm-1m u_i + 1 right)^frac1m-1 - frac1m-1 left( fracm-1m u_i + 1 right)^fracmm-1 right  m  1\n    endcases","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"In particular, for m = 2 we have a simpler formula","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"E_2^*(u) = left fracu2 + 1 right_2^2","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"We now implement E_m^* for m = 1 2.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"E_dual(u, m::Val{1}) = sum(exp.(u))\nfunction E_dual(u, m::Val{2})\n    return dot(u / 2 .+ 1, u / 2 .+ 1)\nend;","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"So, the dual problem we are dealing with reads","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"min_u operatornameOT^*_varepsilon(rho_0 u) + tau E_m^*left( frac-utau-psi right)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"and we can thus set up G_dual_fpe, the dual objective.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"function G_dual_fpe(u, ρ0, τ, ε, K)\n    return OptimalTransport.Dual.ot_entropic_semidual(ρ0, u, ε, K) +\n           τ * E_dual(-u / τ - Ψ, Val(1))\nend;","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Now we set up step as previously, except this time we need to convert from the optimal dual variable u^star to the primal variable rho^star. In the code, this is handled by getprimal_ot_entropic_semidual. We use ReverseDiff in this problem.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"function step(ρ0, τ, ε, K, G)\n    obj = u -> G(u, ρ0, τ, ε, K)\n    opt = optimize(\n        obj,\n        (∇, u) -> ReverseDiff.gradient!(∇, obj, u),\n        zeros(size(ρ0)),\n        LBFGS(),\n        Optim.Options(; iterations=250, g_tol=1e-6),\n    )\n    return OptimalTransport.Dual.getprimal_ot_entropic_semidual(\n        ρ0, Optim.minimizer(opt), ε, K\n    )\nend;","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Now we can solve the dual problem as previously, and we note that the dual formulation is solved an order of magnitude faster than the primal formulation.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"ρ = similar(ρ0, size(ρ0, 1), N)\nρ[:, 1] = ρ0\nfor i in 2:N\n    ρ[:, i] = step(ρ[:, i - 1], τ, ε, K, G_dual_fpe)\nend\ncolors = range(colorant\"red\"; stop=colorant\"blue\", length=N)\nplot(\n    support,\n    ρ;\n    title=raw\"$F(\\rho) = \\langle \\psi, \\rho \\rangle + \\langle \\rho, \\log(\\rho) \\rangle$\",\n    palette=colors,\n    legend=nothing,\n)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"(Image: )","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"Setting m = 2, we can simulate instead the porous medium equation.","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"function G_dual_pme(u, ρ0, τ, ε, K)\n    return OptimalTransport.Dual.ot_entropic_semidual(ρ0, u, ε, K) +\n           τ * E_dual(-u / τ - Ψ, Val(2))\nend\nρ = similar(ρ0, size(ρ0, 1), N)\nρ[:, 1] = ρ0\nfor i in 2:N\n    @info i\n    ρ[:, i] = step(ρ[:, i - 1], τ, ε, K, G_dual_pme)\nend\ncolors = range(colorant\"red\"; stop=colorant\"blue\", length=N)\nplot(\n    support,\n    ρ;\n    title=raw\"$F(\\rho) = \\langle \\psi, \\rho \\rangle + \\langle \\rho, \\rho - 1\\rangle$\",\n    palette=colors,\n    legend=nothing,\n)","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"(Image: )","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"","category":"page"},{"location":"examples/variational/","page":"Variational problems","title":"Variational problems","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"EditURL = \"https://github.com/JuliaOptimalTransport/OptimalTransport.jl/blob/master/examples/OneDimension/script.jl\"","category":"page"},{"location":"examples/OneDimension/#One-Dimensional-Cases","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"","category":"section"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"(Image: )","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"The 1D case in Optimal Transport is a special case where one can easily obtain closed form solutions efficiently when the cost function is convex. In this situation, one does no need to use Linear Programming solvers to obtain the exact solution to the problem.","category":"page"},{"location":"examples/OneDimension/#Packages","page":"One-Dimensional Cases","title":"Packages","text":"","category":"section"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"We load the following packages into our environment:","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"using OptimalTransport\nusing Distances\nusing Distributions\nusing StatsPlots\n\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(1234);","category":"page"},{"location":"examples/OneDimension/#Continuous-Distribution","page":"One-Dimensional Cases","title":"Continuous Distribution","text":"","category":"section"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"In the 1D case, when the source measure mu is continuous and the cost function has the form c(x y) = h(x - y) where h is a convex function, the optimal transport plan is the Monge map","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"T = F_nu^-1 circ F_mu","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"where F_mu is the cumulative distribution function of μ and F_nu^-1 is the quantile function of ν. In this setting, the optimal transport cost can be computed as","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"int_0^1 c(F_mu^-1(x) F_nu^-1(x)) mathrmdx","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"where F_mu^-1 and F_nu^-1 are the quantile functions of μ and ν, respectively.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"We start by defining the distributions.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"μ = Normal(0, 1)\n\nN = 10\nν = Poisson(N);","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"Nest, we define a cost function.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"c(x, y) = (abs(x - y))^2 # could have used `sqeuclidean` from `Distances.jl`\n\nT = ot_plan(c, μ, ν);","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"T is the Monge Map. Let's visualize it.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"p1 = plot(μ; label='μ')\np1 = plot!(ν; marker=:circle, label='ν')\np2 = plot(-2:0.1:2, T(-2:0.1:2); label=\"Monge map\", color=:green, legend=:topleft)\nplot(p1, p2)","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"(Image: )","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"The optimal transport cost can be computed with","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"ot_cost(c, μ, ν)","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"104.72027014853339","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"If instead you want the 2-Wasserstein distance (which is the square root of the optimal transport with the Square Euclidean distatce, then use","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"wasserstein(μ, ν; p=2)","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"10.233292243874079","category":"page"},{"location":"examples/OneDimension/#Finite-Discrete-Distributions","page":"One-Dimensional Cases","title":"Finite Discrete Distributions","text":"","category":"section"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"If the source and target measures are 1D finite discrete distributions (sometimes referred as empirical distributions, or as sample distributions), and if the cost function is convex, then the optimal transport plan can be written as a sorting algorithm, where the utmost left probability mass of the source is transported to the closest probability mass of the target, until everything is transported.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"Define your measures as DiscreteNonParametric, which is a type in Distributions.jl. Also, let's assume both point masses with equal weights and let's use the sqeuclidean function instead of creating our own cost function.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"M = 15\nμ = DiscreteNonParametric(1.5rand(M), fill(1 / M, M))\n\nN = 10\nν = DiscreteNonParametric(1.5rand(N) .+ 2, fill(1 / N, N))\n\nγ = ot_plan(sqeuclidean, μ, ν);","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"This time γ is a sparse matrix containing the transport plan. Let's visualize the results. We create a function curve just as a helper to draw the transport plan.","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"function curve(x1, x2, y1, y2)\n    a = min(y1, y2)\n    b = (y1 - y2 + a * (x1^2 - x2^2)) / (x1 - x2)\n    c = y1 + a * x1^2 - b * x1\n    f(x) = -a * x^2 + b * x + c\n    return f\nend\n\np = plot(μ; marker=:circle, label='μ')\np = plot!(ν; marker=:circle, label='ν', ylims=(0, 0.2))\nfor i in 1:M, j in 1:N\n    if γ[i, j] > 0\n        transport = curve(μ.support[i], ν.support[j], 1 / M, 1 / N)\n        x = range(μ.support[i], ν.support[j]; length=100)\n        p = plot!(x, transport.(x); color=:green, label=nothing, alpha=0.5)\n    end\nend\np","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"(Image: )","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"Again, the optimal transport cost can be calculated with","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"ot_cost(sqeuclidean, μ, ν)","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"3.2925430197981305","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"","category":"page"},{"location":"examples/OneDimension/","page":"One-Dimensional Cases","title":"One-Dimensional Cases","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#OptimalTransport.jl-Documentation","page":"Home","title":"OptimalTransport.jl Documentation","text":"","category":"section"},{"location":"#Exact-optimal-transport-(Kantorovich)-problem","page":"Home","title":"Exact optimal transport (Kantorovich) problem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"OptimalTransport.jl reexports the following functions for exact, i.e., unregularized, optimal transport problems from ExactOptimalTransport.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"emd\nemd2\not_plan\not_plan(::Any, ::ExactOptimalTransport.ContinuousUnivariateDistribution, ::ExactOptimalTransport.UnivariateDistribution)\not_plan(::Any, ::ExactOptimalTransport.DiscreteNonParametric, ::ExactOptimalTransport.DiscreteNonParametric)\not_plan(::ExactOptimalTransport.SqEuclidean, ::ExactOptimalTransport.Normal, ::ExactOptimalTransport.Normal)\not_plan(::ExactOptimalTransport.SqEuclidean, ::ExactOptimalTransport.MvNormal, ::ExactOptimalTransport.MvNormal)\not_cost\not_cost(::Any, ::ExactOptimalTransport.ContinuousUnivariateDistribution, ::ExactOptimalTransport.UnivariateDistribution)\not_cost(::Any, ::ExactOptimalTransport.DiscreteNonParametric, ::ExactOptimalTransport.DiscreteNonParametric)\not_cost(::ExactOptimalTransport.SqEuclidean, ::ExactOptimalTransport.Normal, ::ExactOptimalTransport.Normal)\not_cost(::ExactOptimalTransport.SqEuclidean, ::ExactOptimalTransport.MvNormal, ::ExactOptimalTransport.MvNormal)\nwasserstein\nsquared2wasserstein\ndiscretemeasure","category":"page"},{"location":"#ExactOptimalTransport.emd","page":"Home","title":"ExactOptimalTransport.emd","text":"emd(μ, ν, C, optimizer)\n\nCompute the optimal transport plan γ for the Monge-Kantorovich problem with source histogram μ, target histogram ν, and cost matrix C of size (length(μ), length(ν)) which solves\n\ninf_γ  Π(μ ν) langle γ C rangle\n\nThe corresponding linear programming problem is solved with the user-provided optimizer. Possible choices are Tulip.Optimizer() and Clp.Optimizer() in the Tulip and Clp packages, respectively.\n\n\n\n\n\n","category":"function"},{"location":"#ExactOptimalTransport.emd2","page":"Home","title":"ExactOptimalTransport.emd2","text":"emd2(μ, ν, C, optimizer; plan=nothing)\n\nCompute the optimal transport cost (a scalar) for the Monge-Kantorovich problem with source histogram μ, target histogram ν, and cost matrix C of size (length(μ), length(ν)) which is given by\n\ninf_γ  Π(μ ν) langle γ C rangle\n\nThe corresponding linear programming problem is solved with the user-provided optimizer. Possible choices are Tulip.Optimizer() and Clp.Optimizer() in the Tulip and Clp packages, respectively.\n\nA pre-computed optimal transport plan may be provided.\n\n\n\n\n\n","category":"function"},{"location":"#ExactOptimalTransport.ot_plan","page":"Home","title":"ExactOptimalTransport.ot_plan","text":"ot_plan(c, μ, ν; kwargs...)\n\nCompute the optimal transport plan for the Monge-Kantorovich problem with source and target marginals μ and ν and cost c.\n\nThe optimal transport plan solves\n\ninf_gamma in Pi(mu nu) int c(x y)  mathrmdgamma(x y)\n\nwhere Pi(mu nu) denotes the couplings of mu and nu.\n\nSee also: ot_cost\n\n\n\n\n\n","category":"function"},{"location":"#ExactOptimalTransport.ot_plan-Tuple{Any, Distributions.Distribution{Distributions.Univariate, Distributions.Continuous}, Distributions.UnivariateDistribution}","page":"Home","title":"ExactOptimalTransport.ot_plan","text":"ot_plan(c, μ::ContinuousUnivariateDistribution, ν::UnivariateDistribution)\n\nCompute the optimal transport plan for the Monge-Kantorovich problem with univariate distributions μ and ν as source and target marginals and cost function c of the form c(x y) = h(x - y) where h is a convex function.\n\nIn this setting, the optimal transport plan is the Monge map\n\nT = F_nu^-1 circ F_mu\n\nwhere F_mu is the cumulative distribution function of μ and F_nu^-1 is the quantile function of ν.\n\nSee also: ot_cost, emd\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.ot_plan-Tuple{Any, Distributions.DiscreteNonParametric, Distributions.DiscreteNonParametric}","page":"Home","title":"ExactOptimalTransport.ot_plan","text":"ot_plan(c, μ::DiscreteNonParametric, ν::DiscreteNonParametric)\n\nCompute the optimal transport cost for the Monge-Kantorovich problem with univariate discrete distributions μ and ν as source and target marginals and cost function c of the form c(x y) = h(x - y) where h is a convex function.\n\nIn this setting, the optimal transport plan can be computed analytically. It is returned as a sparse matrix.\n\nSee also: ot_cost, emd\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.ot_plan-Tuple{Distances.SqEuclidean, Distributions.Normal, Distributions.Normal}","page":"Home","title":"ExactOptimalTransport.ot_plan","text":"ot_plan(::SqEuclidean, μ::Normal, ν::Normal)\n\nCompute the optimal transport plan for the Monge-Kantorovich problem with normal distributions μ and ν as source and target marginals and cost function c(x y) = x - y_2^2.\n\nSee also: ot_cost, emd\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.ot_plan-Tuple{Distances.SqEuclidean, Distributions.MvNormal, Distributions.MvNormal}","page":"Home","title":"ExactOptimalTransport.ot_plan","text":"ot_plan(::SqEuclidean, μ::MvNormal, ν::MvNormal)\n\nCompute the optimal transport plan for the Monge-Kantorovich problem with multivariate normal distributions μ and ν as source and target marginals and cost function c(x y) = x - y_2^2.\n\nIn this setting, for mu = mathcalN(m_mu Sigma_mu) and nu = mathcalN(m_nu Sigma_nu), the optimal transport plan is the Monge map\n\nT colon x mapsto m_nu\n+ Sigma_mu^-12\nbig(Sigma_mu^12 Sigma_nu Sigma_mu^12big)^12Sigma_mu^-12\n(x - m_mu)\n\nSee also: ot_cost, emd\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.ot_cost","page":"Home","title":"ExactOptimalTransport.ot_cost","text":"ot_cost(c, μ, ν; kwargs...)\n\nCompute the optimal transport cost for the Monge-Kantorovich problem with source and target marginals μ and ν and cost c.\n\nThe optimal transport cost is the scalar value\n\ninf_gamma in Pi(mu nu) int c(x y)  mathrmdgamma(x y)\n\nwhere Pi(mu nu) denotes the couplings of mu and nu.\n\nSee also: ot_plan\n\n\n\n\n\n","category":"function"},{"location":"#ExactOptimalTransport.ot_cost-Tuple{Any, Distributions.Distribution{Distributions.Univariate, Distributions.Continuous}, Distributions.UnivariateDistribution}","page":"Home","title":"ExactOptimalTransport.ot_cost","text":"ot_cost(\n    c, μ::ContinuousUnivariateDistribution, ν::UnivariateDistribution; plan=nothing\n)\n\nCompute the optimal transport cost for the Monge-Kantorovich problem with univariate distributions μ and ν as source and target marginals and cost function c of the form c(x y) = h(x - y) where h is a convex function.\n\nIn this setting, the optimal transport cost can be computed as\n\nint_0^1 c(F_mu^-1(x) F_nu^-1(x)) mathrmdx\n\nwhere F_mu^-1 and F_nu^-1 are the quantile functions of μ and ν, respectively.\n\nA pre-computed optimal transport plan may be provided.\n\nSee also: ot_plan, emd2\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.ot_cost-Tuple{Any, Distributions.DiscreteNonParametric, Distributions.DiscreteNonParametric}","page":"Home","title":"ExactOptimalTransport.ot_cost","text":"ot_cost(\n    c, μ::DiscreteNonParametric, ν::DiscreteNonParametric; plan=nothing\n)\n\nCompute the optimal transport cost for the Monge-Kantorovich problem with discrete univariate distributions μ and ν as source and target marginals and cost function c of the form c(x y) = h(x - y) where h is a convex function.\n\nIn this setting, the optimal transport cost can be computed analytically.\n\nA pre-computed optimal transport plan may be provided.\n\nSee also: ot_plan, emd2\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.ot_cost-Tuple{Distances.SqEuclidean, Distributions.Normal, Distributions.Normal}","page":"Home","title":"ExactOptimalTransport.ot_cost","text":"ot_cost(::SqEuclidean, μ::Normal, ν::Normal)\n\nCompute the squared 2-Wasserstein distance between univariate normal distributions μ and ν as source and target marginals.\n\nSee also: ot_plan, emd2\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.ot_cost-Tuple{Distances.SqEuclidean, Distributions.MvNormal, Distributions.MvNormal}","page":"Home","title":"ExactOptimalTransport.ot_cost","text":"ot_cost(::SqEuclidean, μ::MvNormal, ν::MvNormal)\n\nCompute the squared 2-Wasserstein distance between normal distributions μ and ν as source and target marginals.\n\nIn this setting, the optimal transport cost can be computed as\n\nW_2^2(mu nu) = m_mu - m_nu ^2 + mathcalB(Sigma_mu Sigma_nu)^2\n\nwhere mu = mathcalN(m_mu Sigma_mu), nu = mathcalN(m_nu Sigma_nu), and mathcalB is the Bures metric.\n\nSee also: ot_plan, emd2\n\n\n\n\n\n","category":"method"},{"location":"#ExactOptimalTransport.wasserstein","page":"Home","title":"ExactOptimalTransport.wasserstein","text":"wasserstein(μ, ν; metric=Euclidean(), p=Val(1), kwargs...)\n\nCompute the p-Wasserstein distance with respect to the metric between measures μ and ν.\n\nOrder p can be provided as a scalar of type Real or as a parameter of a value type Val(p). For certain combinations of metric and p, such as metric=Euclidean() and p=Val(2), the computations are more efficient if p is specified as a value type. The remaining keyword arguments are forwarded to ot_cost.\n\nSee also: squared2wasserstein, ot_cost\n\n\n\n\n\n","category":"function"},{"location":"#ExactOptimalTransport.squared2wasserstein","page":"Home","title":"ExactOptimalTransport.squared2wasserstein","text":"squared2wasserstein(μ, ν; metric=Euclidean(), kwargs...)\n\nCompute the squared 2-Wasserstein distance with respect to the metric between measures μ and ν.\n\nThe remaining keyword arguments are forwarded to ot_cost.\n\nSee also: wasserstein, ot_cost\n\n\n\n\n\n","category":"function"},{"location":"#ExactOptimalTransport.discretemeasure","page":"Home","title":"ExactOptimalTransport.discretemeasure","text":"discretemeasure(\n    support::AbstractVector,\n    probs::AbstractVector{<:Real}=FillArrays.Fill(inv(length(support)), length(support)),\n)\n\nConstruct a finite discrete probability measure with support and corresponding probabilities. If the probability vector argument is not passed, then equal probability is assigned to each entry in the support.\n\nExamples\n\nusing KernelFunctions\n# rows correspond to samples\nμ = discretemeasure(RowVecs(rand(7,3)), normalize!(rand(10),1))\n\n# columns correspond to samples, each with equal probability\nν = discretemeasure(ColVecs(rand(3,12)))\n\nnote: Note\nIf support is a 1D vector, the constructed measure will be sorted, e.g. for mu = discretemeasure([3, 1, 2],[0.5, 0.2, 0.3]), then mu.support will be [1, 2, 3] and mu.p will be [0.2, 0.3, 0.5]. Also, avoid passing 1D distributions as RowVecs(rand(3)) or [[1],[3],[4]], since this will be dispatched to the multivariate case instead of the univariate case for which the algorithm is more efficient.\n\nwarning: Warning\nThis function and in particular its return values are not stable and might be changed in future releases.\n\n\n\n\n\n","category":"function"},{"location":"#Entropically-regularised-optimal-transport","page":"Home","title":"Entropically regularised optimal transport","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"sinkhorn\nsinkhorn2\nsinkhorn_divergence\nsinkhorn_barycenter","category":"page"},{"location":"#OptimalTransport.sinkhorn","page":"Home","title":"OptimalTransport.sinkhorn","text":"sinkhorn(\n    μ, ν, C, ε, alg=SinkhornGibbs();\n    atol=0, rtol=atol > 0 ? 0 : √eps, check_convergence=10, maxiter=1_000,\n)\n\nCompute the optimal transport plan for the entropically regularized optimal transport problem with source and target marginals μ and ν, cost matrix C of size (length(μ), length(ν)), and entropic regularization parameter ε.\n\nThe optimal transport plan γ is of the same size as C and solves\n\ninf_gamma in Pi(mu nu) langle gamma C rangle\n+ varepsilon Omega(gamma)\n\nwhere Omega(gamma) = sum_ij gamma_ij log gamma_ij is the entropic regularization term.\n\nEvery check_convergence steps it is assessed if the algorithm is converged by checking if the iterate of the transport plan G satisfies\n\nisapprox(sum(G; dims=2), μ; atol=atol, rtol=rtol, norm=x -> norm(x, 1))\n\nThe default rtol depends on the types of μ, ν, and C. After maxiter iterations, the computation is stopped.\n\nBatch computations for multiple histograms with a common cost matrix C can be performed by passing μ or ν as matrices whose columns correspond to histograms. It is required that the number of source and target marginals is equal or that a single source or single target marginal is provided (either as matrix or as vector). The optimal transport plans are returned as three-dimensional array where γ[:, :, i] is the optimal transport plan for the ith pair of source and target marginals.\n\nSee also: sinkhorn2\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.sinkhorn2","page":"Home","title":"OptimalTransport.sinkhorn2","text":"sinkhorn2(\n    μ, ν, C, ε, alg=SinkhornGibbs(); regularization=false, plan=nothing, kwargs...\n)\n\nSolve the entropically regularized optimal transport problem with source and target marginals μ and ν, cost matrix C of size (length(μ), length(ν)), and entropic regularization parameter ε, and return the optimal cost.\n\nA pre-computed optimal transport plan may be provided. The other keyword arguments supported here are the same as those in the sinkhorn function.\n\nnote: Note\nAs the sinkhorn2 function in the Python Optimal Transport package, this function returns the optimal transport cost without the regularization term. The cost with the regularization term can be computed by setting regularization=true.\n\nSee also: sinkhorn\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.sinkhorn_divergence","page":"Home","title":"OptimalTransport.sinkhorn_divergence","text":"sinkhorn_divergence(\n    μ::AbstractVecOrMat,\n    ν::AbstractVecOrMat,\n    C,\n    ε,\n    alg::SinkhornDivergence=SinkhornDivergence(\n        SinkhornGibbs(), SymmetricSinkhornGibbs(), SymmetricSinkhornGibbs()\n    );\n    kwargs...,\n)\n\nCompute the Sinkhorn Divergence between finite discrete measures μ and ν with respect to a common cost matrix C, entropic regularization parameter ε and algorithm alg. \n\nIn the default case where regularization = false, the Sinkhorn Divergence is that of [GPC18] and is computed as\n\noperatornameS_ε(μν) = operatornameW_ε(μν)\n- frac12(operatornameW_ε(μμ) + operatornameW_ε(νν))\n\nand operatornameW_ε is defined as\n\noperatornameW_ε(μ ν) = langle C γ^star rangle\n\nwhere γ^star is the entropy-regularised transport plan between μ and ν.  For regularization = true, the Sinkhorn Divergence is that of [FeydyP19] and is computed as above  where operatornameW_ε is replaced by operatornameOT_ε, the entropy-regularised optimal transport  cost with regulariser penalty. \n\nThe default algorithm for computing the term operatornameW_ε(μ ν) is the SinkhornGibbs algorithm. For the terms operatornameW_ε(μ μ) and operatornameW_ε(ν ν), the symmetric fixed point iteration of [FeydyP19] is used.  Alternatively, a pre-computed optimal transport plan between μ and ν may be provided.  \n\n[GPC18]: Aude Genevay, Gabriel Peyré, Marco Cuturi, Learning Generative Models with Sinkhorn Divergences, Proceedings of the Twenty-First International Conference on Artficial Intelligence and Statistics, (AISTATS) 21, 2018\n\n[FeydyP19]: Jean Feydy, Thibault Séjourné, François-Xavier Vialard, Shun-ichi Amari, Alain Trouvé, and Gabriel Peyré. Interpolating between optimal transport and mmd using sinkhorn divergences. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2681–2690. PMLR, 2019.\n\nSee also: sinkhorn2\n\n\n\n\n\nsinkhorn_divergence(\n    μ,\n    ν,\n    Cμν,\n    Cμ,\n    Cν,\n    ε,\n    alg::SinkhornDivergence=SinkhornDivergence(\n        SinkhornGibbs(), SymmetricSinkhornGibbs(), SymmetricSinkhornGibbs()\n    );\n    kwargs...,\n)\n\nCompute the Sinkhorn Divergence between finite discrete measures μ and ν with respect to the precomputed cost matrices Cμν, Cμμ and Cνν, entropic regularization parameter ε and algorithm alg.\n\nA pre-computed optimal transport plan between μ and ν may be provided.\n\nSee also: sinkhorn2, sinkhorn_divergence\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.sinkhorn_barycenter","page":"Home","title":"OptimalTransport.sinkhorn_barycenter","text":"sinkhorn_barycenter(μ, C, ε, w, alg = SinkhornGibbs(); kwargs...)\n\nCompute the Sinkhorn barycenter for a collection of N histograms contained in the columns of μ, for a cost matrix C of size (size(μ, 1), size(μ, 1)), relative weights w of size N, and entropic regularisation parameter ε. Returns the entropically regularised barycenter of the μ, i.e. the histogram ρ of length size(μ, 1) that solves\n\nmin_rho in Sigma sum_i = 1^N w_i operatornameOT_varepsilon(mu_i rho)\n\nwhere operatornameOT_ε(mu nu) = inf_gamma Pi(mu nu) langle gamma C rangle + varepsilon Omega(gamma) is the entropic optimal transport loss with cost C and regularisation epsilon.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"Currently the following variants of the Sinkhorn algorithm are supported:","category":"page"},{"location":"","page":"Home","title":"Home","text":"SinkhornGibbs\nSinkhornStabilized\nSinkhornEpsilonScaling","category":"page"},{"location":"#OptimalTransport.SinkhornGibbs","page":"Home","title":"OptimalTransport.SinkhornGibbs","text":"SinkhornGibbs()\n\nVanilla Sinkhorn algorithm.\n\n\n\n\n\n","category":"type"},{"location":"#OptimalTransport.SinkhornStabilized","page":"Home","title":"OptimalTransport.SinkhornStabilized","text":"SinkhornStabilized(; absorb_tol::Real=1_000)\n\nConstruct a log-domain stabilized Sinkhorn algorithm with absorption tolerance absorb_tol for solving an entropically regularized optimal transport problem.\n\nReferences\n\nSchmitzer, B. (2019). Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems. SIAM Journal on Scientific Computing, 41(3), A1443–A1481.\n\n\n\n\n\n","category":"type"},{"location":"#OptimalTransport.SinkhornEpsilonScaling","page":"Home","title":"OptimalTransport.SinkhornEpsilonScaling","text":"SinkhornEpsilonScaling(algorithm::Sinkhorn; factor=1//2, steps=5)\n\nConstruct an ε-scaling Sinkhorn algorithm for solving an entropically regularized optimal transport problem.\n\nThe function uses the specified Sinkhorn algorithm with steps ε-scaling steps with scaling factor factor. It sequentially solves the entropically regularized optimal transport with regularization parameters\n\nvarepsilon_i = varepsilon lambda^i-k qquad (i = 1ldotsk)\n\nwhere lambda is the scaling factor and k the number of scaling steps.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"The following methods are deprecated and will be removed:","category":"page"},{"location":"","page":"Home","title":"Home","text":"sinkhorn_stabilized\nsinkhorn_stabilized_epsscaling","category":"page"},{"location":"#OptimalTransport.sinkhorn_stabilized","page":"Home","title":"OptimalTransport.sinkhorn_stabilized","text":"sinkhorn_stabilized(μ, ν, C, ε; absorb_tol=1_000, kwargs...)\n\nThis method is deprecated, please use\n\nsinkhorn(\n    μ, ν, C, ε, SinkhornStabilized(; absorb_tol=absorb_tol); kwargs...\n)\n\ninstead.\n\nSee also: sinkhorn, SinkhornStabilized\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.sinkhorn_stabilized_epsscaling","page":"Home","title":"OptimalTransport.sinkhorn_stabilized_epsscaling","text":"sinkhorn_stabilized_epsscaling(\n    μ, ν, C, ε;\n    scaling_factor=1//2, scaling_steps=5, absorb_tol=1_000, kwargs...\n)\n\nThis method is deprecated, please use\n\nsinkhorn(\n    μ,\n    ν,\n    C,\n    ε,\n    SinkhornEpsilonScaling(\n        SinkhornStabilized(; absorb_tol=absorb_tol);\n        factor=scaling_factor,\n        steps=scaling_steps,\n    );\n    kwargs...,\n)\n\ninstead.\n\nSee also: sinkhorn, SinkhornEpsilonScaling\n\n\n\n\n\n","category":"function"},{"location":"#Unbalanced-optimal-transport","page":"Home","title":"Unbalanced optimal transport","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"sinkhorn_unbalanced\nsinkhorn_unbalanced2","category":"page"},{"location":"#OptimalTransport.sinkhorn_unbalanced","page":"Home","title":"OptimalTransport.sinkhorn_unbalanced","text":"sinkhorn_unbalanced(μ, ν, C, λ1::Real, λ2::Real, ε; kwargs...)\n\nCompute the optimal transport plan for the unbalanced entropically regularized optimal transport problem with source and target marginals μ and ν, cost matrix C of size (length(μ), length(ν)), entropic regularization parameter ε, and marginal relaxation terms λ1 and λ2.\n\nThe optimal transport plan γ is of the same size as C and solves\n\ninf_gamma langle gamma C rangle\n+ varepsilon Omega(gamma)\n+ lambda_1 operatornameKL(gamma 1  mu)\n+ lambda_2 operatornameKL(gamma^mathsfT 1  nu)\n\nwhere Omega(gamma) = sum_ij gamma_ij log gamma_ij is the entropic regularization term and operatornameKL is the Kullback-Leibler divergence.\n\nThe keyword arguments supported here are the same as those in the sinkhorn_unbalanced for unbalanced optimal transport problems with general soft marginal constraints.\n\n\n\n\n\nsinkhorn_unbalanced(\n    μ, ν, C, proxdivF1!, proxdivF2!, ε;\n    atol=0, rtol=atol > 0 ? 0 : √eps, check_convergence=10, maxiter=1_000,\n)\n\nCompute the optimal transport plan for the unbalanced entropically regularized optimal transport problem with source and target marginals μ and ν, cost matrix C of size (length(μ), length(ν)), entropic regularization parameter ε, and soft marginal constraints F_1 and F_2 with \"proxdiv\" functions proxdivF! and proxdivG!.\n\nThe optimal transport plan γ is of the same size as C and solves\n\ninf_gamma langle gamma C rangle\n+ varepsilon Omega(gamma)\n+ F_1(gamma 1 mu)\n+ F_2(gamma^mathsfT 1 nu)\n\nwhere Omega(gamma) = sum_ij gamma_ij log gamma_ij is the entropic regularization term and F_1(cdot mu) and F_2(cdot nu) are soft marginal constraints for the source and target marginals.\n\nThe functions proxdivF1!(s, p, ε) and proxdivF2!(s, p, ε) evaluate the \"proxdiv\" functions of F_1(cdot p) and F_2(cdot p) at s for the entropic regularization parameter varepsilon. They have to be mutating and overwrite the first argument s with the result of their computations.\n\nMathematically, the \"proxdiv\" functions are defined as\n\noperatornameproxdiv_F_i(s p varepsilon)\n= operatornameprox^operatornameKL_F_i(cdot p)varepsilon(s) oslash s\n\nwhere oslash denotes element-wise division and operatornameprox_F_i(cdot p)varepsilon^operatornameKL is the proximal operator of F_i(cdot p)varepsilon for the Kullback-Leibler (operatornameKL) divergence.  It is defined as\n\noperatornameprox_F^operatornameKL(x)\n= operatornameargmin_y F(y) + operatornameKL(yx)\n\nand can be computed in closed-form for specific choices of F. For instance, if F(cdot p) = lambda operatornameKL(cdot  p) (lambda  0), then\n\noperatornameprox_F(cdot p)varepsilon^operatornameKL(x)\n= x^fracvarepsilonvarepsilon + lambda p^fraclambdavarepsilon + lambda\n\nwhere all operators are acting pointwise.[CPSV18]\n\nEvery check_convergence steps it is assessed if the algorithm is converged by checking if the iterates of the scaling factor in the current and previous iteration satisfy isapprox(vcat(a, b), vcat(aprev, bprev); atol=atol, rtol=rtol) where a and b are the current iterates and aprev and bprev the previous ones. The default rtol depends on the types of μ, ν, and C. After maxiter iterations, the computation is stopped.\n\n[CPSV18]: Chizat, L., Peyré, G., Schmitzer, B., & Vialard, F.-X. (2018). Scaling algorithms for unbalanced optimal transport problems. Mathematics of Computation, 87(314), 2563–2609.\n\nSee also: sinkhorn_unbalanced2\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.sinkhorn_unbalanced2","page":"Home","title":"OptimalTransport.sinkhorn_unbalanced2","text":"sinkhorn_unbalanced2(μ, ν, C, λ1, λ2, ε; plan=nothing, kwargs...)\nsinkhorn_unbalanced2(μ, ν, C, proxdivF1!, proxdivF2!, ε; plan=nothing, kwargs...)\n\nCompute the optimal transport plan for the unbalanced entropically regularized optimal transport problem with source and target marginals μ and ν, cost matrix C of size (length(μ), length(ν)), entropic regularization parameter ε, and marginal relaxation terms λ1 and λ2 or soft marginal constraints with \"proxdiv\" functions proxdivF1! and proxdivF2!.\n\nA pre-computed optimal transport plan may be provided. The other keyword arguments supported here are the same as those in the sinkhorn_unbalanced for unbalanced optimal transport problems with general soft marginal constraints.\n\nSee also: sinkhorn_unbalanced\n\n\n\n\n\n","category":"function"},{"location":"#Quadratically-regularised-optimal-transport","page":"Home","title":"Quadratically regularised optimal transport","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"quadreg","category":"page"},{"location":"#OptimalTransport.quadreg","page":"Home","title":"OptimalTransport.quadreg","text":"quadreg(μ, ν, C, ε, alg::QuadraticOT; kwargs...)\n\nComputes the optimal transport plan of histograms μ and ν with cost matrix C and quadratic regularization parameter ε. \n\nThe optimal transport plan γ is of the same size as C and solves \n\ninf_gamma in Pi(mu nu) langle gamma C rangle\n+ varepsilon Omega(gamma)\n\nwhere Omega(gamma) = frac12 sum_ij gamma_ij^2 is the quadratic  regularization term.\n\nEvery check_convergence steps it is assessed if the algorithm is converged by checking if the iterate of the transport plan γ satisfies\n\n    norm_diff < max(atol, rtol * max(norm(μ, Inf), norm(ν, Inf)))\n\nwhere\n\n    textnormdiff = max  gamma mathbf1 - mu _infty    gamma^top mathbf1 - nu _infty    \n\nAfter maxiter iterations, the computation is stopped.\n\nNote that unlike in the case of Sinkhorn's algorithm for the entropic regularisation, batch computation of optimal transport is not supported for the quadratic regularisation. \n\nSee also: sinkhorn, QuadraticOTNewton\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"Currently the following algorithms for solving quadratically regularised optimal transport are supported:","category":"page"},{"location":"","page":"Home","title":"Home","text":"QuadraticOTNewton","category":"page"},{"location":"#OptimalTransport.QuadraticOTNewton","page":"Home","title":"OptimalTransport.QuadraticOTNewton","text":"QuadraticOTNewton\n\nSemi-smooth Newton method (Algorithm 2 of Lorenz et al. 2019 [LMM19]) for solving quadratically regularised optimal transport\n\n[LMM19]: Lorenz, Dirk A., Paul Manns, and Christian Meyer. Quadratically regularized optimal transport. Applied Mathematics & Optimization 83.3 (2021): 1919-1949.\n\nSee also: QuadraticOTNewton, quadreg\n\n\n\n\n\n","category":"type"},{"location":"#Dual","page":"Home","title":"Dual","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"OptimalTransport.Dual.ot_entropic_semidual\nOptimalTransport.Dual.ot_entropic_semidual_grad\nOptimalTransport.Dual.getprimal_ot_entropic_semidual\nOptimalTransport.Dual.ot_entropic_dual\nOptimalTransport.Dual.ot_entropic_dual_grad\nOptimalTransport.Dual.getprimal_ot_entropic_dual","category":"page"},{"location":"#OptimalTransport.Dual.ot_entropic_semidual","page":"Home","title":"OptimalTransport.Dual.ot_entropic_semidual","text":"ot_entropic_semidual(μ, v, eps, K)\n\nComputes the semidual (in the second argument) of the entropic optimal transport loss, with source marginal μ, regularization parameter ε, and Gibbs kernel K.  \n\nThat is, \n\n    operatornameOT_varepsilon(mu nu) = inf_gamma in Pi(mu nu) langle gamma C rangle + varepsilon Omega(gamma)\n\nwith Omega(gamma) = sum_ij gamma_ij log gamma_ij, then the semidual in the second argument ν is [Z21]\n\nbeginaligned\n    operatornameOT_varepsilon^*(mu v) = sup_nu langle v nu rangle - operatornameOT_varepsilon(mu nu)   \n    = -varepsilon leftlangle mu logleft( dfracmuK e^vvarepsilon right) - 1rightrangle \nendaligned\n\nNotably, the semidual is computationally advantageous for solving variational problems since it is a smooth and unconstrained function of v since it admits a closed form gradient. See [CP16] for a detailed discussion of dual methods for variational problems in optimal transport. \n\n[CP16]: Cuturi, Marco, and Gabriel Peyré. \"A smoothed dual approach for variational Wasserstein problems.\" SIAM Journal on Imaging Sciences 9.1 (2016): 320-343.\n\n[Z21]: Zhang, Stephen Y. “A Unified Framework for Non-Negative Matrix and Tensor Factorisations with a Smoothed Wasserstein Loss.” ArXiv: Machine Learning, 2021.\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.Dual.ot_entropic_semidual_grad","page":"Home","title":"OptimalTransport.Dual.ot_entropic_semidual_grad","text":"ot_entropic_semidual_grad(μ, v, eps, K)\n\nComputes the gradient with respect to v of the semidual of the entropic optimal transport loss. That is,\n\nnabla_v operatornameOT^*_varepsilon(mu v) = K^top left( dfracmuK e^vvarepsilon right) odot e^vvarepsilon\n\nSee also: ot_entropic_semidual\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.Dual.getprimal_ot_entropic_semidual","page":"Home","title":"OptimalTransport.Dual.getprimal_ot_entropic_semidual","text":"getprimal_ot_entropic_semidual(μ, v, eps, K)\n\nComputes the the primal variable ν corresponding to the dual variable v at optimality. That is,\n\nnu^star = e^v^starvarepsilon odot K^top dfracmuK e^v^starvarepsilon \n\nSee also: ot_entropic_semidual\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.Dual.ot_entropic_dual","page":"Home","title":"OptimalTransport.Dual.ot_entropic_dual","text":"ot_entropic_dual(u, v, eps, K)\n\nComputes the dual in both arguments of entropic optimal transport loss, where u and v are the dual variables associated with the source and target marginals respectively. \n\nThat is,\n\n    beginaligned\n    operatornameOT_varepsilon^*(u v) = sup_mu nu langle u mu rangle + langle v nu rangle - operatornameOT_varepsilon(mu nu)   \n    = varepsilon log langle e^uvarepsilon K e^vvarepsilon rangle \n    endaligned\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.Dual.ot_entropic_dual_grad","page":"Home","title":"OptimalTransport.Dual.ot_entropic_dual_grad","text":"ot_entropic_dual_grad(u, v, eps, K)\n\nComputes the gradient with respect to u and v of the dual of the entropic optimal transport loss. That is,\n\nbeginaligned\nnabla_u operatornameOT^*_varepsilon(u v) = dfrace^uvarepsilon odot K e^vvarepsilonlangle e^uvarepsilon K e^vvarepsilon rangle  \nnabla_v operatornameOT^*_varepsilon(u v) = dfrace^vvarepsilon odot K^top e^uvarepsilonlangle e^vvarepsilon K^top e^uvarepsilon rangle\nendaligned\n\nSee also: ot_entropic_dual\n\n\n\n\n\n","category":"function"},{"location":"#OptimalTransport.Dual.getprimal_ot_entropic_dual","page":"Home","title":"OptimalTransport.Dual.getprimal_ot_entropic_dual","text":"getprimal_ot_entropic_dual(u, v, eps, K)\n\nComputes the the primal variable γ corresponding to the dual variable u, v at optimality. That is,\n\n    gamma = operatornamesoftmax(mathrmdiag(e^uvarepsilon) K mathrmdiag(e^vvarepsilon))\n\nSee also: ot_entropic_dual\n\n\n\n\n\n","category":"function"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"EditURL = \"https://github.com/JuliaOptimalTransport/OptimalTransport.jl/blob/master/examples/basic/script.jl\"","category":"page"},{"location":"examples/basic/#Basics","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"(Image: )","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/basic/#Packages","page":"Basics","title":"Packages","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We load the following packages into our environment:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"using OptimalTransport\nusing Distances\nusing Plots\nusing PythonOT: PythonOT\nusing Tulip\n\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(1234)\n\nconst POT = PythonOT","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"PythonOT","category":"page"},{"location":"examples/basic/#Problem-setup","page":"Basics","title":"Problem setup","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"First, let us initialise two random probability measures mu (source measure) and nu (target measure) in 1D:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"M = 200\nμ = fill(1 / M, M)\nμsupport = rand(M)\n\nN = 250\nν = fill(1 / N, N)\nνsupport = rand(N);","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Now we compute the quadratic cost matrix C_ij =  x_i - x_j _2^2:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"C = pairwise(SqEuclidean(), μsupport', νsupport'; dims=2);","category":"page"},{"location":"examples/basic/#Exact-optimal-transport","page":"Basics","title":"Exact optimal transport","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"The earth mover's distance is defined as the optimal value of the Monge-Kantorovich problem","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"inf_gamma in Pi(mu nu) langle gamma C rangle =\ninf_gamma in Pi(mu nu) sum_i j gamma_ij C_ij","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"where Pi(mu nu) denotes the set of couplings of mu and nu, i.e., the set of joint distributions whose marginals are mu and nu. If C is the quadratic cost matrix, the earth mover's distance is known as the square of the 2-Wasserstein distance.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"The function emd returns the optimal transport plan gamma","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γ = emd(μ, ν, C, Tulip.Optimizer());","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"whilst using emd2 returns the optimal transport cost:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"emd2(μ, ν, C, Tulip.Optimizer())","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"0.0014877814134910635","category":"page"},{"location":"examples/basic/#Entropically-regularised-optimal-transport","page":"Basics","title":"Entropically regularised optimal transport","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We may add an entropy term to the Monge-Kantorovich problem to obtain the entropically regularised optimal transport problem","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"inf_gamma in Pi(mu nu) langle gamma C rangle + varepsilon Omega(gamma)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"where Omega(gamma) = sum_i j gamma_ij log(gamma_ij) is the negative entropy of the coupling gamma and varepsilon controls the strength of the regularisation.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"This problem is strictly convex and admits a very efficient iterative scaling scheme for its solution known as the Sinkhorn algorithm.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We compute the optimal entropically regularised transport plan:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"ε = 0.01\nγ = sinkhorn(μ, ν, C, ε);","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We can check that one obtains the same result with the Python Optimal Transport (POT) package:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γpot = POT.sinkhorn(μ, ν, C, ε)\nnorm(γ - γpot, Inf)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"5.79577190277028e-12","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We can compute the optimal cost (a scalar) of the entropically regularized optimal transport problem with sinkhorn2:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"sinkhorn2(μ, ν, C, ε)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"0.005991722631258635","category":"page"},{"location":"examples/basic/#Quadratically-regularised-optimal-transport","page":"Basics","title":"Quadratically regularised optimal transport","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Instead of the common entropically regularised optimal transport problem, we can solve the quadratically regularised optimal transport problem","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"inf_gamma in Pi(mu nu) langle gamma C rangle + varepsilon frac gamma _F^22","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"One property of the quadratically regularised optimal transport problem is that the resulting transport plan gamma is sparse. We take advantage of this and represent it as a sparse matrix.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"quadreg(μ, ν, C, ε; maxiter=100);","category":"page"},{"location":"examples/basic/#Stabilized-Sinkhorn-algorithm","page":"Basics","title":"Stabilized Sinkhorn algorithm","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"When varepsilon is very small, we can use a log-stabilised version of the Sinkhorn algorithm.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"ε = 0.005\nγ = sinkhorn_stabilized(μ, ν, C, ε; maxiter=5_000);","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Again we can check that the same result is obtained with the POT package:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γ_pot = POT.sinkhorn(μ, ν, C, ε; method=\"sinkhorn_stabilized\", numItermax=5_000)\nnorm(γ - γ_pot, Inf)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"8.68413549620986e-12","category":"page"},{"location":"examples/basic/#Stabilized-Sinkhorn-algorithm-with-\\varepsilon-scaling","page":"Basics","title":"Stabilized Sinkhorn algorithm with varepsilon-scaling","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"In addition to log-stabilisation, we can use varepsilon-scaling:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γ = sinkhorn_stabilized_epsscaling(μ, ν, C, ε; maxiter=5_000);","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"The POT package yields the same result:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γpot = POT.sinkhorn(μ, ν, C, ε; method=\"sinkhorn_epsilon_scaling\", numItermax=5000)\nnorm(γ - γpot, Inf)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"1.665707076935717e-11","category":"page"},{"location":"examples/basic/#Unbalanced-optimal-transport","page":"Basics","title":"Unbalanced optimal transport","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Unbalanced optimal transport deals with general positive measures which do not necessarily have the same total mass. For unbalanced source and target marginals mu and nu and a cost matrix C, entropically regularised unbalanced optimal transport solves","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"inf_gamma geq 0 langle gamma C rangle + varepsilon Omega(gamma) + lambda_1 mathrmKL(gamma 1  mu) + lambda_2 mathrmKL(gamma^mathsfT 1  nu)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"where varepsilon controls the strength of the entropic regularisation, and lambda_1 and lambda_2 control how strongly we enforce the marginal constraints.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We construct two random measures, now with different total masses:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"M = 100\nμ = fill(1 / M, M)\nμsupport = rand(M)\n\nN = 200\nν = fill(1 / M, N)\nνsupport = rand(N);","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We compute the quadratic cost matrix:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"C = pairwise(SqEuclidean(), μsupport', νsupport'; dims=2);","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Now we solve the corresponding unbalanced, entropy-regularised transport problem with varepsilon = 001 and lambda_1 = lambda_2 = 1:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"ε = 0.01\nλ = 1\nγ = sinkhorn_unbalanced(μ, ν, C, λ, λ, ε);","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We check that the result agrees with POT:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γpot = POT.sinkhorn_unbalanced(μ, ν, C, ε, λ)\nnorm(γ - γpot, Inf)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"1.245810654073795e-9","category":"page"},{"location":"examples/basic/#Plots","page":"Basics","title":"Plots","text":"","category":"section"},{"location":"examples/basic/#Entropically-regularised-transport","page":"Basics","title":"Entropically regularised transport","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Let us construct source and target measures again:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"μsupport = νsupport = range(-2, 2; length=100)\nC = pairwise(SqEuclidean(), μsupport', νsupport'; dims=2)\nμ = normalize!(exp.(-μsupport .^ 2 ./ 0.5^2), 1)\nν = normalize!(νsupport .^ 2 .* exp.(-νsupport .^ 2 ./ 0.5^2), 1)\n\nplot(μsupport, μ; label=raw\"$\\mu$\", size=(600, 400))\nplot!(νsupport, ν; label=raw\"$\\nu$\")","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"(Image: )","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Now we compute the entropically regularised transport plan:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γ = sinkhorn(μ, ν, C, 0.01)\nheatmap(\n    μsupport,\n    νsupport,\n    γ;\n    title=\"Entropically regularised optimal transport\",\n    size=(600, 600),\n)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"(Image: )","category":"page"},{"location":"examples/basic/#Quadratically-regularised-transport","page":"Basics","title":"Quadratically regularised transport","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"Notice how the \"edges\" of the transport plan are sharper if we use quadratic regularisation instead of entropic regularisation:","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"γquad = quadreg(μ, ν, C, 5; maxiter=100);\nheatmap(\n    μsupport,\n    νsupport,\n    γquad;\n    title=\"Quadratically regularised optimal transport\",\n    size=(600, 600),\n)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"(Image: )","category":"page"},{"location":"examples/basic/#Sinkhorn-barycenters","page":"Basics","title":"Sinkhorn barycenters","text":"","category":"section"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"For a collection of discrete probability measures mu_i_i=1^N subset mathcalP, cost matrices C_i_i=1^N, and positive weights lambda_i_i=1^N summing to 1, the entropically regularised barycenter in mathcalP is the discrete probability measure mu that solves","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"inf_mu in mathcalP sum_i = 1^N lambda_i operatornameOT_varepsilon(mu mu_i)","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"where operatornameOT_varepsilon(mu mu_i) denotes the entropically regularised optimal transport cost with marginals mu and mu_i, cost matrix C, and entropic regularisation parameter varepsilon.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"We set up two measures and compute the weighted barycenters. We choose weights lambda_1 in 025 05 075.","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"support = range(-1, 1; length=250)\nmu1 = normalize!(exp.(-(support .+ 0.5) .^ 2 ./ 0.1^2), 1)\nmu2 = normalize!(exp.(-(support .- 0.5) .^ 2 ./ 0.1^2), 1)\n\nplt = plot(; size=(800, 400), legend=:outertopright)\nplot!(plt, support, mu1; label=raw\"$\\mu_1$\")\nplot!(plt, support, mu2; label=raw\"$\\mu_2$\")\n\nmu = hcat(mu1, mu2)\nC = pairwise(SqEuclidean(), support'; dims=2)\nfor λ1 in (0.25, 0.5, 0.75)\n    λ2 = 1 - λ1\n    a = sinkhorn_barycenter(mu, C, 0.01, [λ1, λ2], SinkhornGibbs())\n    plot!(plt, support, a; label=\"\\$\\\\mu \\\\quad (\\\\lambda_1 = $λ1)\\$\")\nend\nplt","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"(Image: )","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"","category":"page"},{"location":"examples/basic/","page":"Basics","title":"Basics","text":"This page was generated using Literate.jl.","category":"page"}]
}
