---
title: Computational Optimal Transport with OptimalTransport.jl
author: Stephen Zhang
date: 2 November 2020
---

# Introduction

This file contains a collection of example codes for various functions offered by the [`OptimalTransport.jl`](https://github.com/zsteve/OptimalTransport.jl) package,  
and can be treated as an informal mini-tutorial for using the package. 

## Installation 

Install the package as follows,

```julia
# using Pkg;
# Pkg.add("OptimalTransport")
```
and load into our environment:
```julia
using OptimalTransport
using Distances
using LinearAlgebra
using Plots, LaTeXStrings
using StatsPlots
using Random, Distributions
```

## Problem setup

First, let us initialise two random probability measures $\mu$ (source measure) and $\nu$ (target measure) in 1D:

```julia
N = 200; M = 200
μ_spt = rand(N)
ν_spt = rand(M)
μ = fill(1/N, N)
ν = fill(1/M, M);
```
Now we compute the quadratic cost matrix $C_{ij} = \| x_i - x_j \|^2$

```julia
C = pairwise(SqEuclidean(), μ_spt', ν_spt');
```

## Earth mover's distance (EMD)

The **earth mover's distance** is defined as the optimal value of the Monge-Kantorovich problem:

$$\min_{\gamma \in \Pi(\mu, \nu)} \langle \gamma, C \rangle = \min_{\gamma \in \Pi(\mu, \nu)} \sum_{i, j} \gamma_{ij} C_{ij}$$

where $\Pi(\mu, \nu)$ denotes the set of joint distributions whose marginals agree with $\mu$ and $\nu$. 
In the case where $C$ is the quadratic cost, this corresponds to what is known as the **2-Wasserstein distance**.

**N.B.** At the moment, this functionality is available through PyCall and the Python OT library.


Using `emd()` returns the optimal transport *plan* $\gamma$:
```julia
γ = OptimalTransport.emd(μ, ν, C);
```
whilst using `emd2()` returns the optimal transport *cost* at the minimum:
```julia
OptimalTransport.emd2(μ, ν, C)
```

## Entropically regularised transport using Sinkhorn scaling algorithm

We may add an entropy term to the Monge-Kantorovich problem to obtain the **entropically regularised** optimal transport problem:

$$\min_{\gamma \in \Pi(\mu, \nu)} \langle \gamma, C \rangle - \epsilon H(\gamma)$$

where $H(\gamma) = -\sum_{i, j} \gamma_{ij} \log(\gamma_{ij})$ is the entropy of the coupling $\gamma$ and $\epsilon$ controls the strength of regularisation.

This problem is *strictly convex* and admits a very efficient iterative scaling scheme for its solution known as the Sinkhorn algorithm [Peyre 2019]. 

Compute the transport plan using native Julia vs. POT 

```julia
ϵ = 0.01
γ_ = OptimalTransport.pot_sinkhorn(μ, ν, C, ϵ);
γ = OptimalTransport.sinkhorn(μ, ν, C, ϵ)
```
Now we can check that both implementations arrive at the same result:

```julia
norm(γ - γ_, Inf)
```

Again, we can compute the optimal value (a scalar) of the entropic OT problem using `sinkhorn2()`:
```julia
OptimalTransport.sinkhorn2(μ, ν, C, ϵ)
``` 
## Quadratically regularised transport.

Try computing the transport plan for the same problem, this time using a quadratic regularisation [Lorenz 2019] instead of the more common entropic regulariser term. We solve the problem

$$\min_{\gamma \in \Pi(\mu, \nu)} \langle \gamma, C \rangle + \epsilon \frac{\| \gamma \|_F^2}{2}$$

One property of quadratically regularised OT is that the resulting transport plan $\gamma$ is *sparse*. We take advantage of this and represent it as a sparse matrix. 

```julia
γ = OptimalTransport.quadreg(μ, ν, C, ϵ);
γ
```

## Stabilized Sinkhorn

This is a log-stabilised version of the Sinkhorn algorithm which is useful when $\epsilon$ is very small [Schmitzer 2019]

```julia
ϵ = 0.005
γ =  sinkhorn_stabilized(μ, ν, C, ϵ, max_iter = 5000);
γ_ = pot_sinkhorn(μ, ν, C, ϵ, method = "sinkhorn_stabilized", max_iter = 5000);
norm(γ - γ_, Inf) # Check that we get the same result as POT
```

## Stabilized Sinkhorn eps-scaling

In addition to log-stabilisation, we can additionally use $\epsilon$-scaling [Schmitzer 2019]

```julia
γ =  sinkhorn_stabilized_epsscaling(μ, ν, C, ϵ, max_iter = 5000)
γ_ = pot_sinkhorn(μ, ν, C, ϵ, method = "sinkhorn_epsilon_scaling", max_iter = 5000)
norm(γ - γ_, Inf) # Check that we get the same result as POT
```

## Unbalanced transport 

Unbalanced transport was introduced by [Chizat 2019] to interpolate between general positive measures which do not have the same total mass. That is, for $\mu, \nu \in \mathcal{M}_+$ and a cost function (e.g. quadratic) $C$, we solve the following problem:

$$\min_{\gamma \ge 0} \epsilon \mathrm{KL}(\gamma | \exp(-C/\epsilon)) + \lambda_1 \mathrm{KL}(\gamma_1 | \mu) + \lambda_2 \mathrm{KL}(\gamma_2 | \nu)$$ 

where $\epsilon$ controls the strength of entropic regularisation, and $\lambda_1, \lambda_2$ control how strongly we enforce the marginal constraints. 

We construct two random measures, now with different total masses:
```julia
N = 100; M = 200
μ_spt = rand(N)
ν_spt = rand(M)
μ = fill(1/N, N)
ν = fill(1/N, M);
```

Set up quadratic cost matrix:

```julia
C = pairwise(SqEuclidean(), μ_spt', ν_spt')
```

Now we solve the corresponding unbalanced, entropy-regularised transport problem. 

```julia
ϵ = 0.01
λ = 1.0
γ_ = pot_sinkhorn_unbalanced(μ, ν, C, ϵ, λ)
γ = sinkhorn_unbalanced(μ, ν, C, λ, λ, ϵ)
```

Check agreement with POT:

```julia
norm(γ - γ_, Inf) # Check that we get the same result as POT
```

## Example plots

### Entropically regularised transport

Let's construct source and target measures again
```julia
μ_spt = ν_spt = LinRange(-2, 2, 100)
C = pairwise(Euclidean(), μ_spt', ν_spt').^2
μ = exp.((-(μ_spt).^2)/0.5^2)
μ /= sum(μ)
ν = ν_spt.^2 .*exp.((-(ν_spt).^2)/0.5^2)
ν /= sum(ν)

plot(μ_spt, μ, label = L"\mu")
plot!(ν_spt, ν, label = L"\nu")
current()
```

Now compute the entropic transport plan:

```julia
γ = OptimalTransport.sinkhorn(μ, ν, C, 0.01)
heatmap(γ, title = "Entropically regularised OT", size = (500, 500))
current()
```

### Quadratically regularised transport

Using a quadratic regularisation, notice how the 'edges' of the transport plan here are sharper compared to the entropic OT transport plan.

```julia
γ_quad = Matrix(OptimalTransport.quadreg(μ, ν, C, 5, maxiter = 500))
heatmap(γ_quad, title = "Quadratically regularised OT", size = (500, 500))
current()
```

## Sinkhorn barycenters

For a collection of probability measures $\{ \mu_i \}_{i = 1}^N$ and corresponding cost matrices $C_i$, we define the **barycenter** to be the measure $\mu$ that solves the following:

$$\min_{\mu \text{ a distribution}} \sum_{i = 1}^N \lambda_i \mathrm{entropicOT}^{\epsilon}_{C_i}(\mu, \mu_i)$$

where $\mathrm{entropicOT}^\epsilon_{C_i}(\mu, \mu_i)$ denotes the entropic optimal transport cost between measures $(\mu, \mu_i)$ with cost $C_i$ and entropic regularisation strength $\epsilon$.

For instance, below we set up two measures $\mu_0$ and $\mu_1$ and compute the weighted barycenters. The weights are $(w, 1-w)$ where $w = 0.25, 0.5, 0.75$.

```julia
spt = LinRange(-1, 1, 250)
f(x, σ) = exp.(-x.^2/σ^2)
normalize(x) = x./sum(x)
mu_all = hcat([normalize(f(spt .- z, 0.1)) for z in [-0.5, 0.5]]...)'
C_all = [pairwise(SqEuclidean(), spt', spt') for i = 1:size(mu_all, 1)]

plot(size = (400, 200));
plot!(spt, mu_all[1, :], label = L"\mu_0")
plot!(spt, mu_all[2, :], label = L"\mu_1")
for s = [0.25, 0.5, 0.75] 
    a = sinkhorn_barycenter(mu_all, C_all, 0.01, [s, 1-s]; max_iter = 1000);
    plot!(spt, a, label= latexstring("\\mu_{$s}"))
end
current()
```
